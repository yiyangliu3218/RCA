#!/usr/bin/env python3
"""
TKG Export Module - å¯¼å‡ºåˆ†é’Ÿåˆ‡ç‰‡ä¸ºç»Ÿä¸€æ ¼å¼
å°† KG-RCA2 çš„åˆ†é’Ÿçº§ TKG å¯¼å‡ºä¸º LLM-DA å¯ç”¨çš„æ ‡å‡†æ ¼å¼

å…³é”®ä¿®æ­£ï¼š
1. æ—¶é—´æˆ³æ¥æºï¼šä¼˜å…ˆä»èŠ‚ç‚¹/è¾¹å±æ€§è¯»å–çœŸå®æ—¶é—´ï¼Œè€Œéç›®å½•å
2. æœåŠ¡èŠ‚ç‚¹ï¼šä¸è®¾ç½®äº‹ä»¶æ—¶é—´ï¼Œåªä¿ç•™åˆ†é’Ÿæ—¶é—´ç”¨äºæ‹¼çª—
3. å±æ€§ç±»å‹è½¬æ¢ï¼šå¼ºåˆ¶è½¬æ¢ GraphML å­—ç¬¦ä¸²å±æ€§ä¸ºæ­£ç¡®ç±»å‹
4. èŠ‚ç‚¹IDå”¯ä¸€æ€§ï¼šè§„èŒƒåŒ–äº‹ä»¶èŠ‚ç‚¹IDæ ¼å¼
5. æ—¥æœŸè¡¥å…¨ï¼šæ”¯æŒå¤šå¤©æ•°æ®çš„ä¸‰å±‚è§£æ
6. æ–‡ä»¶æ ¼å¼å…¼å®¹ï¼šæ”¯æŒå¤šç§å›¾è°±æ ¼å¼
"""
import os
import json
import pandas as pd
import networkx as nx
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime
import glob
import re

def coerce_attr(value: Any) -> Union[str, int, float, bool, datetime, None]:
    """
    å¼ºåˆ¶è½¬æ¢ GraphML å±æ€§ç±»å‹
    å°è¯•è½¬æ¢ä¸º float / int / bool / datetimeï¼Œå¤±è´¥åˆ™ä¿ç•™å­—ç¬¦ä¸²
    """
    if value is None or value == "":
        return None
    
    if isinstance(value, (int, float, bool)):
        return value
    
    if isinstance(value, str):
        # å°è¯•è½¬æ¢ä¸ºæ•°å€¼
        try:
            # æ•´æ•°
            if value.isdigit() or (value.startswith('-') and value[1:].isdigit()):
                return int(value)
        except:
            pass
        
        try:
            # æµ®ç‚¹æ•°
            return float(value)
        except:
            pass
        
        # å¸ƒå°”å€¼
        if value.lower() in ('true', 'false'):
            return value.lower() == 'true'
        
        # æ—¶é—´æˆ³ï¼ˆå¤šç§æ ¼å¼ï¼‰
        try:
            # ISO æ ¼å¼
            return pd.to_datetime(value)
        except:
            pass
        
        try:
            # Unix æ—¶é—´æˆ³
            if value.replace('.', '').replace('-', '').isdigit():
                return pd.to_datetime(float(value), unit='s')
        except:
            pass
        
        # ä¿ç•™å­—ç¬¦ä¸²
        return value
    
    return value

def parse_timestamp_from_path(path: Path) -> Tuple[Optional[datetime], Optional[float]]:
    """
    ä»è·¯å¾„è§£ææ—¶é—´æˆ³
    æ”¯æŒ dataset/date/time/ ä¸‰å±‚ç»“æ„
    è¿”å› (datetimeå¯¹è±¡, timestampç§’æ•°)
    """
    try:
        parts = path.parts
        
        # æŸ¥æ‰¾æ—¶é—´ç›¸å…³éƒ¨åˆ†
        date_part = None
        time_part = None
        
        for part in parts:
            # åŒ¹é…æ—¥æœŸæ ¼å¼ YYYY-MM-DD
            if re.match(r'\d{4}-\d{2}-\d{2}', part):
                date_part = part
            # åŒ¹é…æ—¶é—´æ ¼å¼ HH-MM-SS
            elif re.match(r'\d{2}-\d{2}-\d{2}', part):
                time_part = part
        
        if date_part and time_part:
            # ç»„åˆæ—¥æœŸå’Œæ—¶é—´
            dt_str = f"{date_part}T{time_part.replace('-', ':')}"
            dt = pd.to_datetime(dt_str)
            return dt, dt.timestamp()
        elif time_part:
            # åªæœ‰æ—¶é—´ï¼Œä½¿ç”¨ä»Šå¤©æ—¥æœŸ
            today = datetime.now().strftime('%Y-%m-%d')
            dt_str = f"{today}T{time_part.replace('-', ':')}"
            dt = pd.to_datetime(dt_str)
            return dt, dt.timestamp()
        
    except Exception as e:
        print(f"âš ï¸ è§£æè·¯å¾„æ—¶é—´æˆ³å¤±è´¥ {path}: {e}")
    
    return None, None

def normalize_node_id(node_id: str, node_type: str, attrs: Dict[str, Any]) -> str:
    """
    è§„èŒƒåŒ–èŠ‚ç‚¹IDï¼Œç¡®ä¿å…¨å±€å”¯ä¸€æ€§
    äº‹ä»¶èŠ‚ç‚¹ï¼š{kind}:{svc}:{name}:{iso_ts}
    æœåŠ¡èŠ‚ç‚¹ï¼šsvc:{name}
    """
    if node_type == "Service":
        service = attrs.get('service', 'unknown')
        return f"svc:{service}"
    
    elif node_type in ("MetricEvent", "LogEvent"):
        service = attrs.get('service', 'unknown')
        event_ts = attrs.get('event_ts')
        
        if node_type == "MetricEvent":
            metric = attrs.get('metric', 'unknown')
            name = metric
        else:  # LogEvent
            template_id = attrs.get('template_id', 'unknown')
            name = template_id
        
        if event_ts:
            if isinstance(event_ts, datetime):
                iso_ts = event_ts.isoformat()
            else:
                iso_ts = str(event_ts)
        else:
            iso_ts = "unknown"
        
        kind = "met" if node_type == "MetricEvent" else "log"
        return f"{kind}:{service}:{name}:{iso_ts}"
    
    else:
        # å…¶ä»–ç±»å‹ä¿æŒåŸæ ·
        return node_id

def load_graph_file(file_path: Path) -> Optional[nx.MultiDiGraph]:
    """
    åŠ è½½å›¾è°±æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼
    """
    try:
        if file_path.suffix == '.graphml':
            return nx.read_graphml(file_path)
        elif file_path.suffix == '.pkl':
            return nx.read_gpickle(file_path)
        elif file_path.suffix == '.gexf':
            return nx.read_gexf(file_path)
        elif file_path.suffix == '.json':
            with open(file_path, 'r') as f:
                data = json.load(f)
            return nx.node_link_graph(data)
        else:
            print(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            return None
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å›¾è°±æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None

def export_tkg_slices(output_dir: str, merged_dir: str) -> dict:
    """
    è¯»å– KG-RCA2/outputs çš„æ¯åˆ†é’Ÿåˆ‡ç‰‡(G_t.*)ï¼Œå½’ä¸€åŒ–å­˜ä¸º
      nodes: {id,type,service,metric,template_id,event_ts,minute_ts,attrs...}
      edges: {src,dst,type,weight,event_ts,minute_ts}
    ç”Ÿæˆç´¢å¼•(index.jsonï¼šæ—¶é—´èŒƒå›´ã€åˆ†é’Ÿåˆ—è¡¨)ã€‚
    è¿”å› {'nodes_path':..., 'edges_path':..., 'index_path':...}
    """
    print(f"ğŸ”„ å¯¼å‡º TKG åˆ‡ç‰‡ä» {output_dir} åˆ° {merged_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(merged_dir, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰å›¾è°±æ–‡ä»¶
    graph_files = []
    for dataset_dir in Path(output_dir).glob("*"):
        if not dataset_dir.is_dir():
            continue
            
        for date_dir in dataset_dir.glob("*"):
            if not date_dir.is_dir():
                continue
                
            for time_dir in date_dir.glob("*"):
                if not time_dir.is_dir():
                    continue
                
                # æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
                for ext in ['.graphml', '.pkl', '.gexf', '.json']:
                    files = list(time_dir.glob(f"*{ext}"))
                    if files:
                        graph_files.append((time_dir, files[0]))
                        break
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(graph_files)} ä¸ªå›¾è°±æ–‡ä»¶")
    
    if not graph_files:
        print("âš ï¸ æœªæ‰¾åˆ°å›¾è°±æ–‡ä»¶")
        return {"nodes_path": None, "edges_path": None, "index_path": None}
    
    # å¤„ç†æ¯ä¸ªå›¾è°±æ–‡ä»¶
    all_nodes = []
    all_edges = []
    time_index = []
    
    for time_dir, graph_file in graph_files:
        try:
            # è¯»å–å›¾è°±
            G = load_graph_file(graph_file)
            if G is None:
                continue
                
            print(f"ğŸ“Š å¤„ç†å›¾è°±: {graph_file} ({G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹)")
            
            # ä»è·¯å¾„è§£æåˆ†é’Ÿæ—¶é—´æˆ³
            minute_dt, minute_ts = parse_timestamp_from_path(time_dir)
            if minute_ts is None:
                minute_ts = 0
                minute_dt = datetime.fromtimestamp(0)
            
            # å¤„ç†èŠ‚ç‚¹
            for node_id, data in G.nodes(data=True):
                # å¼ºåˆ¶è½¬æ¢å±æ€§ç±»å‹
                attrs = {k: coerce_attr(v) for k, v in data.items()}
                
                # ç¡®å®šèŠ‚ç‚¹ç±»å‹
                node_type = attrs.get('type', 'unknown')
                
                # æå–äº‹ä»¶æ—¶é—´ï¼ˆä¼˜å…ˆä»å±æ€§è¯»å–ï¼‰
                event_ts = None
                for ts_key in ['event_ts', 'timestamp', 'ts', 'time']:
                    if ts_key in attrs and attrs[ts_key] is not None:
                        event_ts = attrs[ts_key]
                        break
                
                # è§„èŒƒåŒ–èŠ‚ç‚¹ID
                normalized_id = normalize_node_id(node_id, node_type, attrs)
                
                # æ„å»ºèŠ‚ç‚¹ä¿¡æ¯
                node_info = {
                    'id': normalized_id,
                    'node_type': node_type,
                    'minute_ts': minute_ts,
                    'event_ts': event_ts.timestamp() if isinstance(event_ts, datetime) else event_ts,
                }
                
                # æ·»åŠ å†—ä½™å­—æ®µï¼ˆä¸ºåç»­ walk åŠ é€Ÿï¼‰
                if 'service' in attrs:
                    node_info['service'] = attrs['service']
                if 'metric' in attrs:
                    node_info['metric'] = attrs['metric']
                if 'template_id' in attrs:
                    node_info['template_id'] = attrs['template_id']
                if 'zscore' in attrs:
                    node_info['zscore'] = attrs['zscore']
                if 'severity' in attrs:
                    node_info['severity'] = attrs['severity']
                
                # æœåŠ¡èŠ‚ç‚¹ä¸è®¾ç½®äº‹ä»¶æ—¶é—´
                if node_type == "Service":
                    node_info['event_ts'] = None
                
                all_nodes.append(node_info)
            
            # å¤„ç†è¾¹
            for src, dst, data in G.edges(data=True):
                # å¼ºåˆ¶è½¬æ¢å±æ€§ç±»å‹
                attrs = {k: coerce_attr(v) for k, v in data.items()}
                
                # è§„èŒƒåŒ–æºå’Œç›®æ ‡èŠ‚ç‚¹ID
                src_attrs = G.nodes[src]
                dst_attrs = G.nodes[dst]
                src_type = src_attrs.get('type', 'unknown')
                dst_type = dst_attrs.get('type', 'unknown')
                
                normalized_src = normalize_node_id(src, src_type, src_attrs)
                normalized_dst = normalize_node_id(dst, dst_type, dst_attrs)
                
                # ç¡®å®šè¾¹çš„äº‹ä»¶æ—¶é—´
                edge_event_ts = None
                if attrs.get('type') == 'precedes':
                    # precedes è¾¹ä½¿ç”¨è¾ƒæ—©çš„äº‹ä»¶æ—¶é—´
                    src_event_ts = None
                    dst_event_ts = None
                    
                    for ts_key in ['event_ts', 'timestamp', 'ts', 'time']:
                        if ts_key in src_attrs:
                            src_event_ts = coerce_attr(src_attrs[ts_key])
                            break
                        if ts_key in dst_attrs:
                            dst_event_ts = coerce_attr(dst_attrs[ts_key])
                            break
                    
                    if src_event_ts and dst_event_ts:
                        edge_event_ts = min(
                            src_event_ts.timestamp() if isinstance(src_event_ts, datetime) else src_event_ts,
                            dst_event_ts.timestamp() if isinstance(dst_event_ts, datetime) else dst_event_ts
                        )
                
                # æ„å»ºè¾¹ä¿¡æ¯
                edge_info = {
                    'src': normalized_src,
                    'dst': normalized_dst,
                    'edge_type': attrs.get('type', 'unknown'),
                    'weight': float(attrs.get('weight', 1.0)),
                    'minute_ts': minute_ts,
                    'event_ts': edge_event_ts,
                }
                
                # æ·»åŠ å†—ä½™å­—æ®µ
                edge_info['src_type'] = src_type
                edge_info['dst_type'] = dst_type
                
                all_edges.append(edge_info)
            
            # è®°å½•æ—¶é—´ç´¢å¼•
            time_index.append({
                'time_str': time_dir.name,
                'minute_ts': minute_ts,
                'minute_dt': minute_dt.isoformat(),
                'nodes_count': G.number_of_nodes(),
                'edges_count': G.number_of_edges(),
                'graph_path': str(graph_file)
            })
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å›¾è°±å¤±è´¥ {graph_file}: {e}")
            continue
    
    # æŒ‰æ—¶é—´æ’åº
    time_index.sort(key=lambda x: x['minute_ts'])
    
    # ä¿å­˜èŠ‚ç‚¹æ•°æ®
    nodes_df = pd.DataFrame(all_nodes)
    nodes_path = os.path.join(merged_dir, "nodes.parquet")
    nodes_df.to_parquet(nodes_path, index=False)
    print(f"ğŸ“¤ ä¿å­˜èŠ‚ç‚¹æ•°æ®: {nodes_path} ({len(all_nodes)} èŠ‚ç‚¹)")
    
    # ä¿å­˜è¾¹æ•°æ®
    edges_df = pd.DataFrame(all_edges)
    edges_path = os.path.join(merged_dir, "edges.parquet")
    edges_df.to_parquet(edges_path, index=False)
    print(f"ğŸ“¤ ä¿å­˜è¾¹æ•°æ®: {edges_path} ({len(all_edges)} è¾¹)")
    
    # ä¿å­˜ç´¢å¼•
    index_path = os.path.join(merged_dir, "index.json")
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump({
            'time_range': {
                'start': time_index[0]['minute_ts'] if time_index else 0,
                'end': time_index[-1]['minute_ts'] if time_index else 0
            },
            'minutes': time_index,
            'total_nodes': len(all_nodes),
            'total_edges': len(all_edges)
        }, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“¤ ä¿å­˜ç´¢å¼•: {index_path}")
    
    # éªŒè¯æ—¶é—´çº¦æŸï¼ˆä½¿ç”¨ event_tsï¼‰
    print("ğŸ” éªŒè¯æ—¶é—´çº¦æŸ...")
    precedes_edges = [e for e in all_edges if e['edge_type'] == 'precedes']
    valid_precedes = 0
    
    # æ„å»ºèŠ‚ç‚¹IDåˆ°èŠ‚ç‚¹çš„æ˜ å°„
    node_map = {n['id']: n for n in all_nodes}
    
    for edge in precedes_edges:
        src_node = node_map.get(edge['src'])
        dst_node = node_map.get(edge['dst'])
        
        if src_node and dst_node:
            src_ts = src_node.get('event_ts') or src_node.get('minute_ts', 0)
            dst_ts = dst_node.get('event_ts') or dst_node.get('minute_ts', 0)
            
            if src_ts < dst_ts:
                valid_precedes += 1
    
    print(f"âœ… æ—¶é—´çº¦æŸéªŒè¯: {valid_precedes}/{len(precedes_edges)} precedes è¾¹æ»¡è¶³æ—¶é—´é€’å¢")
    
    return {
        'nodes_path': nodes_path,
        'edges_path': edges_path,
        'index_path': index_path
    }

def validate_tkg_export(nodes_path: str, edges_path: str, index_path: str) -> bool:
    """éªŒè¯å¯¼å‡ºçš„ TKG æ•°æ®"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not all(os.path.exists(p) for p in [nodes_path, edges_path, index_path]):
            print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        # è¯»å–æ•°æ®
        nodes_df = pd.read_parquet(nodes_path)
        edges_df = pd.read_parquet(edges_path)
        
        with open(index_path, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        print(f"ğŸ“Š éªŒè¯ç»“æœ:")
        print(f"   èŠ‚ç‚¹æ•°: {len(nodes_df)}")
        print(f"   è¾¹æ•°: {len(edges_df)}")
        print(f"   æ—¶é—´èŒƒå›´: {index_data['time_range']}")
        print(f"   åˆ†é’Ÿæ•°: {len(index_data['minutes'])}")
        
        # æ£€æŸ¥æ—¶é—´æˆ³å¯è§£ææ€§
        valid_event_timestamps = 0
        valid_minute_timestamps = 0
        
        for _, row in nodes_df.iterrows():
            if pd.notna(row.get('event_ts')) and row.get('event_ts', 0) > 0:
                valid_event_timestamps += 1
            if pd.notna(row.get('minute_ts')) and row.get('minute_ts', 0) > 0:
                valid_minute_timestamps += 1
        
        print(f"   æœ‰æ•ˆäº‹ä»¶æ—¶é—´æˆ³: {valid_event_timestamps}/{len(nodes_df)}")
        print(f"   æœ‰æ•ˆåˆ†é’Ÿæ—¶é—´æˆ³: {valid_minute_timestamps}/{len(nodes_df)}")
        
        # æ£€æŸ¥èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ
        node_types = nodes_df['node_type'].value_counts()
        print(f"   èŠ‚ç‚¹ç±»å‹åˆ†å¸ƒ: {dict(node_types)}")
        
        # æ£€æŸ¥è¾¹ç±»å‹åˆ†å¸ƒ
        edge_types = edges_df['edge_type'].value_counts()
        print(f"   è¾¹ç±»å‹åˆ†å¸ƒ: {dict(edge_types)}")
        
        # æ£€æŸ¥æ—¶é—´çº¦æŸ
        precedes_edges = edges_df[edges_df['edge_type'] == 'precedes']
        if len(precedes_edges) > 0:
            valid_precedes = 0
            for _, edge in precedes_edges.iterrows():
                src_node = nodes_df[nodes_df['id'] == edge['src']]
                dst_node = nodes_df[nodes_df['id'] == edge['dst']]
                
                if len(src_node) > 0 and len(dst_node) > 0:
                    src_ts = src_node.iloc[0].get('event_ts') or src_node.iloc[0].get('minute_ts', 0)
                    dst_ts = dst_node.iloc[0].get('event_ts') or dst_node.iloc[0].get('minute_ts', 0)
                    
                    if src_ts < dst_ts:
                        valid_precedes += 1
            
            print(f"   æ—¶é—´çº¦æŸéªŒè¯: {valid_precedes}/{len(precedes_edges)} precedes è¾¹æ»¡è¶³æ—¶é—´é€’å¢")
        
        return True
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # æµ‹è¯•å¯¼å‡º
    output_dir = "outputs"
    merged_dir = "LLM-DA/datasets/tkg"
    
    result = export_tkg_slices(output_dir, merged_dir)
    print(f"ğŸ“‹ å¯¼å‡ºç»“æœ: {result}")
    
    if result['nodes_path']:
        validate_tkg_export(result['nodes_path'], result['edges_path'], result['index_path'])
