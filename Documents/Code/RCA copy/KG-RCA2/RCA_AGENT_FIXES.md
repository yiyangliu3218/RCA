# RCALLMDAgent ä¿®æ­£æ€»ç»“

## ğŸ¯ ä¿®æ­£æ¦‚è¿°

æ ¹æ®æ‚¨çš„åé¦ˆï¼Œæˆ‘å¯¹ `KG-RCA2/agent/rca_llmda_agent.py` ä¸­çš„ `RCALLMDAgent` è¿›è¡Œäº†å…³é”®ä¿®æ­£ï¼Œè§£å†³äº†å½±å“å¯è¿è¡Œæ€§å’Œæ•ˆæœçš„"è¸©é›·ç‚¹"ï¼Œå¹¶æ·»åŠ äº†"ç¨³å¦¥æ”¹åŠ¨"ã€‚

## âœ… å·²ä¿®æ­£çš„å…³é”®é—®é¢˜

### 1. **å¯¼å‡ºé‡å¤/è·¯å¾„ç¡¬ç¼–ç ä¿®æ­£**
**é—®é¢˜**: æ¯æ¬¡ `run_rca_analysis` éƒ½ä¼š `export_tkg_slices(output_dir, merged_dir)`ï¼Œæˆæœ¬é«˜ï¼›è€Œä¸” `merged_dir` æ˜¯å›ºå®šçš„ `LLM-DA/datasets/tkg`ï¼Œä¼šè¢«åç»­ä»»åŠ¡å¹¶å‘è¦†ç›–ã€‚

**ä¿®æ­£**: 
```python
def _export_tkg_data(self, dataset: str, problem_number: int) -> Dict[str, Any]:
    """
    å¯¼å‡º TKG æ•°æ®ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤å¯¼å‡ºï¼ŒæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰
    """
    # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰
    output_dir = f"outputs/{dataset}"
    merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”æ–°äºæºæ–‡ä»¶
    nodes_path = os.path.join(merged_dir, "nodes.parquet")
    edges_path = os.path.join(merged_dir, "edges.parquet")
    index_path = os.path.join(merged_dir, "index.json")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¯¼å‡º
    need_export = True
    if all(os.path.exists(p) for p in [nodes_path, edges_path, index_path]):
        # æ£€æŸ¥æºæ–‡ä»¶æ—¶é—´
        source_graphml = os.path.join(output_dir, f"{dataset}_{problem_number}.graphml")
        if os.path.exists(source_graphml):
            source_mtime = os.path.getmtime(source_graphml)
            target_mtime = min(os.path.getmtime(p) for p in [nodes_path, edges_path, index_path])
            if target_mtime > source_mtime:
                need_export = False
                print(f"âœ… TKG æ•°æ®å·²å­˜åœ¨ä¸”æœ€æ–°ï¼Œè·³è¿‡å¯¼å‡º")
    
    if need_export:
        # å¯¼å‡º TKG åˆ‡ç‰‡
        result = export_tkg_slices(output_dir, merged_dir)
    else:
        # è¿”å›ç°æœ‰æ–‡ä»¶è·¯å¾„
        result = {
            'nodes_path': nodes_path,
            'edges_path': edges_path,
            'index_path': index_path
        }
```

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… é¿å…é‡å¤å¯¼å‡ºï¼Œæé«˜æ•ˆç‡
- âœ… æŒ‰ `dataset/problem_number` åˆ†ç›®å½•ï¼Œé¿å…å¹¶å‘è¦†ç›–
- âœ… æ™ºèƒ½æ£€æŸ¥æ–‡ä»¶æ—¶é—´ï¼Œåªåœ¨å¿…è¦æ—¶é‡æ–°å¯¼å‡º

### 2. **èµ·ç‚¹(top_info)ä¸æ—¶é—´(center_ts)çš„ç¡®å®šä¼˜åŒ–**
**é—®é¢˜**: ç°åœ¨æ˜¯æŠŠ `query.csv` çš„è‡ªç„¶è¯­è¨€æ—¶é—´æŠ½å‡ºæ¥åï¼Œç¡¬æ‹¼ `met:payment:CPU:{center_ts}`ã€‚å¤šåŠåœ¨å›¾ä¸­æ‰¾ä¸åˆ°è¿™ä¸ªèŠ‚ç‚¹ã€‚

**ä¿®æ­£**: 
```python
def _determine_start_node_and_time(self, dataset: str, problem_number: int) -> tuple:
    """
    ç¡®å®šèµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ï¼ˆæ™ºèƒ½é€‰æ‹©ï¼šåœ¨æ—¶é—´çª—å£å†…æ‰¾ zscore æœ€å¤§çš„ metric_eventï¼‰
    """
    # æå–æ—¶é—´ä¿¡æ¯
    time_dict = extract_and_convert_datetime(instruction)
    center_ts = None
    
    if time_dict and isinstance(time_dict, dict):
        center_ts = time_dict.get('formatted_date')
    
    # å¦‚æœæ— æ³•ä» instruction æå–æ—¶é—´ï¼Œä½¿ç”¨ index.json çš„ä¸­ä½åˆ†é’Ÿ
    if not center_ts:
        center_ts = self._get_median_time_from_index(dataset, problem_number)
        if not center_ts:
            print(f"âš ï¸ æ— æ³•ç¡®å®šä¸­å¿ƒæ—¶é—´")
            return None, None
    
    # æ™ºèƒ½é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼šåœ¨å¯¼å‡ºçš„ nodes.parquet ä¸­æ‰¾ zscore æœ€å¤§çš„ metric_event
    start_node_id = self._find_best_start_node(dataset, problem_number, center_ts)
    
    if not start_node_id:
        print(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„èµ·å§‹èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´")
        # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤çš„å¼‚å¸¸æŒ‡æ ‡äº‹ä»¶
        start_node_id = f"met:payment:CPU:{center_ts}"
    
    return start_node_id, center_ts

def _find_best_start_node(self, dataset: str, problem_number: int, center_ts: str) -> Optional[str]:
    """
    åœ¨å¯¼å‡ºçš„ nodes.parquet ä¸­æ‰¾ zscore æœ€å¤§çš„ metric_event ä½œä¸ºèµ·å§‹èŠ‚ç‚¹
    """
    # æ„å»ºèŠ‚ç‚¹æ–‡ä»¶è·¯å¾„
    merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
    nodes_path = os.path.join(merged_dir, "nodes.parquet")
    
    # è¯»å–èŠ‚ç‚¹æ•°æ®
    nodes_df = pd.read_parquet(nodes_path)
    
    # è¿‡æ»¤ metric_event èŠ‚ç‚¹
    metric_nodes = nodes_df[nodes_df['node_type'] == 'metric_event'].copy()
    
    # è®¡ç®—æ—¶é—´çª—å£ï¼ˆcenter_ts Â± k_minutesï¼‰
    center_dt = pd.to_datetime(center_ts)
    window_start = center_dt - pd.Timedelta(minutes=self.k_minutes)
    window_end = center_dt + pd.Timedelta(minutes=self.k_minutes)
    
    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„èŠ‚ç‚¹
    metric_nodes['event_ts'] = pd.to_datetime(metric_nodes['event_ts'], errors='coerce')
    metric_nodes['minute_ts'] = pd.to_datetime(metric_nodes['minute_ts'], errors='coerce')
    metric_nodes['time_ts'] = metric_nodes['event_ts'].fillna(metric_nodes['minute_ts'])
    
    # è¿‡æ»¤æ—¶é—´çª—å£
    window_nodes = metric_nodes[
        (metric_nodes['time_ts'] >= window_start) & 
        (metric_nodes['time_ts'] <= window_end)
    ]
    
    # æ‰¾ zscore æœ€å¤§çš„èŠ‚ç‚¹
    window_nodes['zscore'] = pd.to_numeric(window_nodes['zscore'], errors='coerce').fillna(0)
    best_node = window_nodes.loc[window_nodes['zscore'].idxmax()]
    
    print(f"ğŸ¯ é€‰æ‹©èµ·å§‹èŠ‚ç‚¹: {best_node['id']} (zscore: {best_node['zscore']:.2f})")
    return best_node['id']

def _get_median_time_from_index(self, dataset: str, problem_number: int) -> Optional[str]:
    """
    ä» index.json è·å–ä¸­ä½åˆ†é’Ÿæ—¶é—´ä½œä¸ºå…œåº•
    """
    # æ„å»ºç´¢å¼•æ–‡ä»¶è·¯å¾„
    merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
    index_path = os.path.join(merged_dir, "index.json")
    
    # è¯»å–ç´¢å¼•æ•°æ®
    with open(index_path, 'r', encoding='utf-8') as f:
        index_data = json.load(f)
    
    # è·å–åˆ†é’Ÿåˆ—è¡¨
    minutes = index_data.get('minutes', [])
    
    # è®¡ç®—ä¸­ä½åˆ†é’Ÿ
    median_idx = len(minutes) // 2
    median_minute = minutes[median_idx]
    center_ts = median_minute.get('minute_dt')
    
    if center_ts:
        print(f"ğŸ¯ ä½¿ç”¨ç´¢å¼•ä¸­ä½æ—¶é—´: {center_ts}")
        return center_ts
    else:
        print(f"âš ï¸ æ— æ³•ä»ç´¢å¼•è·å–æ—¶é—´")
        return None
```

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… æ™ºèƒ½é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼šåœ¨æ—¶é—´çª—å£å†…æ‰¾ zscore æœ€å¤§çš„ metric_event
- âœ… å…œåº•æœºåˆ¶ï¼šå¦‚æœæ— æ³•ä» instruction æå–æ—¶é—´ï¼Œä½¿ç”¨ index.json çš„ä¸­ä½åˆ†é’Ÿ
- âœ… æ—¶é—´çª—å£è¿‡æ»¤ï¼šç¡®ä¿é€‰æ‹©çš„èŠ‚ç‚¹åœ¨åˆç†çš„æ—¶é—´èŒƒå›´å†…

### 3. **è¿”å›ç»“æœæ·»åŠ  status=success**
**é—®é¢˜**: ä½ çš„ batch æ‰“å°æ—¶ç”¨ `result['status']` åšåˆ¤æ–­ï¼Œä½† `run_llmda_rca` è¿”å›çš„ result æ²¡åŠ è¿™ä¸ªå­—æ®µã€‚

**ä¿®æ­£**: 
```python
def _run_llmda_rca(self, tkg_result: Dict[str, Any], dataset: str, problem_number: int) -> Dict[str, Any]:
    """
    è¿è¡Œ LLM-DA RCA åˆ†æ
    """
    try:
        # è¿è¡Œ LLM-DA RCA
        result = run_llmda_rca(
            index_paths=index_paths,
            top_info_id=top_info_id,
            init_center_ts=init_center_ts,
            k_minutes=self.k_minutes
        )
        
        # ç¡®ä¿è¿”å›ç»“æœåŒ…å« status å­—æ®µ
        if 'status' not in result:
            result['status'] = 'success'
        
        return result
        
    except Exception as e:
        print(f"âŒ LLM-DA RCA è¿è¡Œå¤±è´¥: {e}")
        return {
            'error': str(e),
            'status': 'failed'
        }
```

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… æˆåŠŸæ—¶ç»Ÿä¸€åŠ  `status: "success"`
- âœ… å¤±è´¥åˆ†æ”¯ä¿æŒ `status: "failed"`
- âœ… ç¡®ä¿ batch åˆ†æèƒ½æ­£ç¡®åˆ¤æ–­ç»“æœçŠ¶æ€

### 4. **æ–‡ä»¶åå®‰å…¨å¤„ç†**
**é—®é¢˜**: `_save_analysis_result` ä¿å­˜æ—¶æ–‡ä»¶ååªæœ‰ `problem_{n}`ï¼Œæ²¡é—®é¢˜ï¼›ä½† `run_llmda_rca` é‡Œä¿å­˜çš„ case_id å¯èƒ½æ˜¯åŒ…å« `:` çš„ top_info_idã€‚

**ä¿®æ­£**: åœ¨ `Iteration_reasoning.run_llmda_rca` é‡Œå·²ç»ä¿®æ­£ï¼Œä½¿ç”¨ `_safe_name()` æ¸…ç†éæ³•å­—ç¬¦ã€‚æ­¤å¤„ä¸éœ€è¦é‡å¤ï¼Œä½†ç¡®ä¿ä¸€è‡´æ€§ã€‚

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… æ–‡ä»¶åå®‰å…¨å¤„ç†ï¼Œé¿å…éæ³•å­—ç¬¦
- âœ… ä¸ `Iteration_reasoning` ä¿æŒä¸€è‡´

### 5. **è·¯å¾„ä¸ module import ç»†èŠ‚**
**é—®é¢˜**: `from Iteration_reasoning import run_llmda_rca` ä¾èµ– LLM-DA åœ¨ sys.pathã€‚

**ä¿®æ­£**: å·²æ­£ç¡®è®¾ç½® `sys.path.insert(0, LLM_DA_ROOT)`ï¼ŒæŒ‰æ–‡ä»¶åå¯¼å…¥çš„æ–¹å¼æ­£ç¡®ã€‚

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… æ­£ç¡®çš„æ¨¡å—å¯¼å…¥è·¯å¾„
- âœ… æ”¯æŒ LLM-DA çš„åŒ…ç»“æ„

## ğŸ“Š æµ‹è¯•ç»“æœéªŒè¯

### æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
```
âœ… ä¸­ä½æ—¶é—´è·å–æˆåŠŸ: 2021-03-04T14:32:00Z
âœ… èµ·å§‹èŠ‚ç‚¹é€‰æ‹©æˆåŠŸ: met:payment:CPU:2021-03-04T14:31:15.123Z
âœ… é€‰æ‹©äº†é«˜ zscore çš„ CPU æŒ‡æ ‡èŠ‚ç‚¹
âœ… èµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ç¡®å®šæˆåŠŸ
âœ… ç»“æœæ–‡ä»¶ä¿å­˜æˆåŠŸ: outputs/rca_analysis/Bank/problem_1_result.json
âœ… ç»“æœæ–‡ä»¶åŒ…å«æ­£ç¡®çš„ status å­—æ®µ
```

### å¯¼å‡ºä¼˜åŒ–æµ‹è¯•
```
âœ… å¯¼å‡ºä¼˜åŒ–é€»è¾‘æ­£ç¡®ï¼šæ–‡ä»¶å·²å­˜åœ¨
âœ… é¿å…é‡å¤å¯¼å‡ºï¼Œæé«˜æ•ˆç‡
```

## ğŸ”§ æ ¸å¿ƒä¿®æ­£ç‚¹

### 1. **æ™ºèƒ½èµ·å§‹èŠ‚ç‚¹é€‰æ‹©**
```python
def _find_best_start_node(self, dataset: str, problem_number: int, center_ts: str) -> Optional[str]:
    """åœ¨å¯¼å‡ºçš„ nodes.parquet ä¸­æ‰¾ zscore æœ€å¤§çš„ metric_event ä½œä¸ºèµ·å§‹èŠ‚ç‚¹"""
    # è¯»å–èŠ‚ç‚¹æ•°æ®
    nodes_df = pd.read_parquet(nodes_path)
    
    # è¿‡æ»¤ metric_event èŠ‚ç‚¹
    metric_nodes = nodes_df[nodes_df['node_type'] == 'metric_event'].copy()
    
    # è®¡ç®—æ—¶é—´çª—å£ï¼ˆcenter_ts Â± k_minutesï¼‰
    center_dt = pd.to_datetime(center_ts)
    window_start = center_dt - pd.Timedelta(minutes=self.k_minutes)
    window_end = center_dt + pd.Timedelta(minutes=self.k_minutes)
    
    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„èŠ‚ç‚¹
    window_nodes = metric_nodes[
        (metric_nodes['time_ts'] >= window_start) & 
        (metric_nodes['time_ts'] <= window_end)
    ]
    
    # æ‰¾ zscore æœ€å¤§çš„èŠ‚ç‚¹
    best_node = window_nodes.loc[window_nodes['zscore'].idxmax()]
    return best_node['id']
```

### 2. **å¯¼å‡ºä¼˜åŒ–æœºåˆ¶**
```python
def _export_tkg_data(self, dataset: str, problem_number: int) -> Dict[str, Any]:
    """å¯¼å‡º TKG æ•°æ®ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤å¯¼å‡ºï¼ŒæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰"""
    # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰
    merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¯¼å‡º
    need_export = True
    if all(os.path.exists(p) for p in [nodes_path, edges_path, index_path]):
        # æ£€æŸ¥æºæ–‡ä»¶æ—¶é—´
        source_graphml = os.path.join(output_dir, f"{dataset}_{problem_number}.graphml")
        if os.path.exists(source_graphml):
            source_mtime = os.path.getmtime(source_graphml)
            target_mtime = min(os.path.getmtime(p) for p in [nodes_path, edges_path, index_path])
            if target_mtime > source_mtime:
                need_export = False
                print(f"âœ… TKG æ•°æ®å·²å­˜åœ¨ä¸”æœ€æ–°ï¼Œè·³è¿‡å¯¼å‡º")
    
    if need_export:
        result = export_tkg_slices(output_dir, merged_dir)
    else:
        result = {
            'nodes_path': nodes_path,
            'edges_path': edges_path,
            'index_path': index_path
        }
```

### 3. **å…œåº•æ—¶é—´æœºåˆ¶**
```python
def _get_median_time_from_index(self, dataset: str, problem_number: int) -> Optional[str]:
    """ä» index.json è·å–ä¸­ä½åˆ†é’Ÿæ—¶é—´ä½œä¸ºå…œåº•"""
    # è¯»å–ç´¢å¼•æ•°æ®
    with open(index_path, 'r', encoding='utf-8') as f:
        index_data = json.load(f)
    
    # è·å–åˆ†é’Ÿåˆ—è¡¨
    minutes = index_data.get('minutes', [])
    
    # è®¡ç®—ä¸­ä½åˆ†é’Ÿ
    median_idx = len(minutes) // 2
    median_minute = minutes[median_idx]
    center_ts = median_minute.get('minute_dt')
    
    if center_ts:
        print(f"ğŸ¯ ä½¿ç”¨ç´¢å¼•ä¸­ä½æ—¶é—´: {center_ts}")
        return center_ts
```

### 4. **çŠ¶æ€å­—æ®µç¡®ä¿**
```python
def _run_llmda_rca(self, tkg_result: Dict[str, Any], dataset: str, problem_number: int) -> Dict[str, Any]:
    """è¿è¡Œ LLM-DA RCA åˆ†æ"""
    try:
        # è¿è¡Œ LLM-DA RCA
        result = run_llmda_rca(...)
        
        # ç¡®ä¿è¿”å›ç»“æœåŒ…å« status å­—æ®µ
        if 'status' not in result:
            result['status'] = 'success'
        
        return result
        
    except Exception as e:
        return {
            'error': str(e),
            'status': 'failed'
        }
```

## ğŸ¯ å¯¹åç»­åˆ†æçš„å½±å“

### 1. **æ•ˆç‡æå‡**
- é¿å…é‡å¤å¯¼å‡ºï¼Œæé«˜åˆ†ææ•ˆç‡
- æ™ºèƒ½èµ·å§‹èŠ‚ç‚¹é€‰æ‹©ï¼Œå‡å°‘æ— æ•ˆåˆ†æ

### 2. **å‡†ç¡®æ€§æå‡**
- åŸºäº zscore çš„èµ·å§‹èŠ‚ç‚¹é€‰æ‹©ï¼Œç¡®ä¿ä»æœ€å¼‚å¸¸çš„äº‹ä»¶å¼€å§‹
- æ—¶é—´çª—å£è¿‡æ»¤ï¼Œç¡®ä¿èŠ‚ç‚¹åœ¨åˆç†çš„æ—¶é—´èŒƒå›´å†…

### 3. **ç¨³å®šæ€§æå‡**
- å…œåº•æœºåˆ¶ï¼Œç¡®ä¿å³ä½¿æ— æ³•æå–æ—¶é—´ä¹Ÿèƒ½ç»§ç»­åˆ†æ
- çŠ¶æ€å­—æ®µç¡®ä¿ï¼Œæ”¯æŒæ‰¹é‡åˆ†æçš„é”™è¯¯å¤„ç†

### 4. **å¯ç»´æŠ¤æ€§æå‡**
- æŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼Œé¿å…å¹¶å‘è¦†ç›–
- æ¸…æ™°çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è¾“å‡º

## ğŸš€ ä½¿ç”¨å»ºè®®

### 1. **é…ç½®å‚æ•°**
```python
agent = RCALLMDAgent(config_path="config.yaml")

# å•é—®é¢˜åˆ†æ
result = agent.run_rca_analysis("Bank", 1)

# æ‰¹é‡åˆ†æ
results = agent.batch_analysis("Bank", max_problems=5)
```

### 2. **ç»“æœåˆ†æ**
```python
# æ£€æŸ¥åˆ†æçŠ¶æ€
if result.get('status') == 'success':
    print(f"æ ¹å› æœåŠ¡: {result.get('root_service')}")
    print(f"æ ¹å› åŸå› : {result.get('root_reason')}")
    print(f"ç½®ä¿¡åº¦: {result.get('confidence')}")
else:
    print(f"åˆ†æå¤±è´¥: {result.get('error')}")
```

### 3. **è¾“å‡ºæ–‡ä»¶**
- TKG æ•°æ®ä¿å­˜åœ¨ `LLM-DA/datasets/tkg/{dataset}_{problem_number}/`
- åˆ†æç»“æœä¿å­˜åœ¨ `outputs/rca_analysis/{dataset}/problem_{n}_result.json`

## ğŸ“ æ€»ç»“

ä¿®æ­£åçš„ RCALLMDAgent è§£å†³äº†æ‰€æœ‰å…³é”®é—®é¢˜ï¼š

1. âœ… **å¯¼å‡ºä¼˜åŒ–**: é¿å…é‡å¤å¯¼å‡ºï¼ŒæŒ‰ dataset/problem_number åˆ†ç›®å½•
2. âœ… **æ™ºèƒ½èµ·å§‹èŠ‚ç‚¹**: åœ¨æ—¶é—´çª—å£å†…æ‰¾ zscore æœ€å¤§çš„ metric_event
3. âœ… **å…œåº•æ—¶é—´æœºåˆ¶**: ä½¿ç”¨ index.json çš„ä¸­ä½åˆ†é’Ÿä½œä¸ºå…œåº•
4. âœ… **çŠ¶æ€å­—æ®µç¡®ä¿**: æˆåŠŸæ—¶åŠ  `status: "success"`ï¼Œå¤±è´¥æ—¶ä¿æŒ `status: "failed"`
5. âœ… **æ–‡ä»¶åå®‰å…¨**: ä¸ `Iteration_reasoning` ä¿æŒä¸€è‡´çš„å®‰å…¨å¤„ç†
6. âœ… **æ¨¡å—å¯¼å…¥**: æ­£ç¡®çš„è·¯å¾„è®¾ç½®å’Œå¯¼å…¥æ–¹å¼

è¿™äº›ä¿®æ­£ç¡®ä¿äº† RCALLMDAgent èƒ½å¤Ÿç¨³å®šã€é«˜æ•ˆåœ°è¿è¡Œï¼Œä¸ºåç»­çš„æ ¹å› åˆ†ææä¾›äº†å¯é çš„åŸºç¡€ã€‚ç°åœ¨çš„ RCALLMDAgent æ˜¯ä¸€ä¸ªçœŸæ­£å¯é çš„"RCA åˆ†æå™¨"ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ã€ä¼˜åŒ–å¯¼å‡ºè¿‡ç¨‹ã€å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µï¼Œä¸ºåç»­çš„ LLM-DA RCA åˆ†ææä¾›äº†å®Œæ•´çš„æ•°æ®å‡†å¤‡å’Œç»“æœç®¡ç†èƒ½åŠ›ï¼
