#!/usr/bin/env python3
"""
ç®€åŒ–æµ‹è¯•ä¿®æ­£åçš„ RCALLMDAgent æ ¸å¿ƒåŠŸèƒ½
"""
import os
import sys
import json
import pandas as pd
from pathlib import Path

# Add project paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

def test_core_functions():
    """æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯• RCALLMDAgent æ ¸å¿ƒåŠŸèƒ½")
    print("=" * 50)
    
    try:
        from agent.rca_llmda_agent import RCALLMDAgent
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
        
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dirs = [
            "data/Bank",
            "LLM-DA/datasets/tkg/Bank_1"
        ]
        
        for dir_path in test_dirs:
            os.makedirs(dir_path, exist_ok=True)
        
        # åˆ›å»ºæµ‹è¯• query.csv
        query_data = [
            {
                "problem_number": 1,
                "instruction": "At 2021-03-04 14:31:00, the payment service experienced high CPU usage and memory spikes, leading to HTTP 500 errors. Please analyze the root cause."
            }
        ]
        
        query_df = pd.DataFrame(query_data)
        query_df.to_csv("data/Bank/query.csv", index=False)
        
        # åˆ›å»ºæµ‹è¯• nodes.parquet
        nodes_data = [
            # æœåŠ¡èŠ‚ç‚¹
            {"id": "svc:payment", "node_type": "service", "service": "payment", 
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": None},
            
            # æŒ‡æ ‡äº‹ä»¶èŠ‚ç‚¹ï¼ˆé«˜ zscoreï¼‰
            {"id": "met:payment:CPU:2021-03-04T14:31:15.123Z", "node_type": "metric_event", 
             "service": "payment", "metric": "CPU", "zscore": 3.5, "value": 95.2,
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": "2021-03-04T14:31:15.123Z"},
            {"id": "met:payment:Memory:2021-03-04T14:31:45.456Z", "node_type": "metric_event", 
             "service": "payment", "metric": "Memory", "zscore": 2.8, "value": 88.5,
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": "2021-03-04T14:31:45.456Z"},
        ]
        
        nodes_df = pd.DataFrame(nodes_data)
        nodes_df.to_parquet("LLM-DA/datasets/tkg/Bank_1/nodes.parquet", index=False)
        
        # åˆ›å»ºæµ‹è¯• index.json
        index_data = {
            "time_range": {
                "start": 1614868260.0,
                "end": 1614868320.0
            },
            "minutes": [
                {
                    "time_str": "14-31-00",
                    "minute_ts": 1614868260.0,
                    "minute_dt": "2021-03-04T14:31:00Z",
                    "nodes_count": 4,
                    "edges_count": 3,
                    "graph_path": "test.graphml"
                },
                {
                    "time_str": "14-32-00", 
                    "minute_ts": 1614868320.0,
                    "minute_dt": "2021-03-04T14:32:00Z",
                    "nodes_count": 1,
                    "edges_count": 2,
                    "graph_path": "test.graphml"
                }
            ],
            "total_nodes": 5,
            "total_edges": 6
        }
        
        with open("LLM-DA/datasets/tkg/Bank_1/index.json", 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
        
        print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
        
        # åˆ›å»º Agent
        agent = RCALLMDAgent()
        
        # æµ‹è¯• 1: ä¸­ä½æ—¶é—´è·å–
        print(f"\nğŸ” æµ‹è¯• 1: ä¸­ä½æ—¶é—´è·å–...")
        median_time = agent._get_median_time_from_index("Bank", 1)
        
        if median_time:
            print(f"   âœ… ä¸­ä½æ—¶é—´è·å–æˆåŠŸ: {median_time}")
        else:
            print(f"   âŒ ä¸­ä½æ—¶é—´è·å–å¤±è´¥")
            return False
        
        # æµ‹è¯• 2: æ™ºèƒ½èµ·å§‹èŠ‚ç‚¹é€‰æ‹©
        print(f"\nğŸ” æµ‹è¯• 2: æ™ºèƒ½èµ·å§‹èŠ‚ç‚¹é€‰æ‹©...")
        start_node_id = agent._find_best_start_node("Bank", 1, median_time)
        
        if start_node_id:
            print(f"   âœ… èµ·å§‹èŠ‚ç‚¹é€‰æ‹©æˆåŠŸ: {start_node_id}")
            
            # æ£€æŸ¥æ˜¯å¦é€‰æ‹©äº† zscore æœ€å¤§çš„èŠ‚ç‚¹
            if "met:payment:CPU" in start_node_id:
                print(f"   âœ… é€‰æ‹©äº†é«˜ zscore çš„ CPU æŒ‡æ ‡èŠ‚ç‚¹")
            else:
                print(f"   âš ï¸ æœªé€‰æ‹©é¢„æœŸçš„ CPU æŒ‡æ ‡èŠ‚ç‚¹")
        else:
            print(f"   âŒ èµ·å§‹èŠ‚ç‚¹é€‰æ‹©å¤±è´¥")
            return False
        
        # æµ‹è¯• 3: å®Œæ•´èµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ç¡®å®š
        print(f"\nğŸ” æµ‹è¯• 3: å®Œæ•´èµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ç¡®å®š...")
        start_node_id, center_ts = agent._determine_start_node_and_time("Bank", 1)
        
        if start_node_id and center_ts:
            print(f"   âœ… èµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ç¡®å®šæˆåŠŸ")
            print(f"   èµ·å§‹èŠ‚ç‚¹: {start_node_id}")
            print(f"   ä¸­å¿ƒæ—¶é—´: {center_ts}")
        else:
            print(f"   âŒ èµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ç¡®å®šå¤±è´¥")
            return False
        
        # æµ‹è¯• 4: ç»“æœä¿å­˜
        print(f"\nğŸ” æµ‹è¯• 4: ç»“æœä¿å­˜...")
        mock_result = {
            "root_service": "payment",
            "root_reason": "metric_anomaly",
            "root_time": "2021-03-04T14:31:15.123Z",
            "evidence_paths": [
                ["met:payment:CPU:2021-03-04T14:31:15.123Z", "met:payment:Memory:2021-03-04T14:31:45.456Z"]
            ],
            "rules_used": ["('metric_event', 'precedes', 'metric_event')"],
            "confidence": 0.85,
            "analysis_time": "2021-03-04T14:35:00Z",
            "status": "success"
        }
        
        agent._save_analysis_result(mock_result, "Bank", 1)
        
        # æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶
        result_file = "outputs/rca_analysis/Bank/problem_1_result.json"
        if os.path.exists(result_file):
            print(f"   âœ… ç»“æœæ–‡ä»¶ä¿å­˜æˆåŠŸ: {result_file}")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            with open(result_file, 'r', encoding='utf-8') as f:
                saved_result = json.load(f)
            
            if saved_result.get('status') == 'success':
                print(f"   âœ… ç»“æœæ–‡ä»¶åŒ…å«æ­£ç¡®çš„ status å­—æ®µ")
            else:
                print(f"   âš ï¸ ç»“æœæ–‡ä»¶ç¼ºå°‘ status å­—æ®µ")
        else:
            print(f"   âŒ ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_export_optimization():
    """æµ‹è¯•å¯¼å‡ºä¼˜åŒ–åŠŸèƒ½"""
    print(f"\nğŸ§ª æµ‹è¯•å¯¼å‡ºä¼˜åŒ–åŠŸèƒ½")
    print("=" * 40)
    
    try:
        from agent.rca_llmda_agent import RCALLMDAgent
        
        agent = RCALLMDAgent()
        
        # æµ‹è¯•å¯¼å‡ºä¼˜åŒ–é€»è¾‘ï¼ˆä¸å®é™…å¯¼å‡ºï¼‰
        print(f"ğŸ“Š æµ‹è¯•å¯¼å‡ºä¼˜åŒ–é€»è¾‘...")
        
        # æ¨¡æ‹Ÿæ–‡ä»¶å­˜åœ¨æ£€æŸ¥
        merged_dir = f"LLM-DA/datasets/tkg/Bank_1"
        nodes_path = os.path.join(merged_dir, "nodes.parquet")
        edges_path = os.path.join(merged_dir, "edges.parquet")
        index_path = os.path.join(merged_dir, "index.json")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        files_exist = all(os.path.exists(p) for p in [nodes_path, edges_path, index_path])
        
        if files_exist:
            print(f"   âœ… å¯¼å‡ºä¼˜åŒ–é€»è¾‘æ­£ç¡®ï¼šæ–‡ä»¶å·²å­˜åœ¨")
        else:
            print(f"   âš ï¸ éƒ¨åˆ†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦å¯¼å‡º")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¼å‡ºä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def cleanup_test_data():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    import shutil
    
    test_dirs = [
        "data/Bank",
        "LLM-DA/datasets/tkg/Bank_1",
        "outputs/rca_analysis"
    ]
    
    for dir_path in test_dirs:
        if os.path.exists(dir_path):
            shutil.rmtree(dir_path)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®: {dir_path}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª ä¿®æ­£åçš„ RCALLMDAgent æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    try:
        # æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
        core_success = test_core_functions()
        
        # æµ‹è¯•å¯¼å‡ºä¼˜åŒ–
        export_success = test_export_optimization()
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cleanup_test_data()
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   æ ¸å¿ƒåŠŸèƒ½: {'âœ… é€šè¿‡' if core_success else 'âŒ å¤±è´¥'}")
        print(f"   å¯¼å‡ºä¼˜åŒ–: {'âœ… é€šè¿‡' if export_success else 'âŒ å¤±è´¥'}")
        
        overall_success = core_success and export_success
        print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ: {'âœ… é€šè¿‡' if overall_success else 'âŒ å¤±è´¥'}")
        
        return overall_success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        cleanup_test_data()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
