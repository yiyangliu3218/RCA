# LLM-DA RCA ä¿®æ­£æ€»ç»“

## ğŸ¯ ä¿®æ­£æ¦‚è¿°

æ ¹æ®æ‚¨çš„åé¦ˆï¼Œæˆ‘å¯¹ `LLM-DA/Iteration_reasoning.py` ä¸­çš„ `run_llmda_rca` å’Œç›¸å…³å‡½æ•°è¿›è¡Œäº†å…³é”®ä¿®æ­£ï¼Œè§£å†³äº†å½±å“å¯è¿è¡Œæ€§å’Œæ•ˆæœçš„ç¡¬ä¼¤é—®é¢˜ã€‚

## âœ… å·²ä¿®æ­£çš„å…³é”®é—®é¢˜

### 1. **æ—¶é—´å­—æ®µä¸ä¸€è‡´ä¿®æ­£**
**é—®é¢˜**: CMRW/Loader é‡‡ç”¨ `event_ts / minute_ts`ï¼Œä½† `_score_root_causes`ã€`_learn_rules_from_paths` ç­‰ä»ç”¨ `timestamp` å’Œ `z`ã€‚

**ä¿®æ­£**: 
```python
def _node_time(G, nid):
    """è·å–èŠ‚ç‚¹æ—¶é—´ï¼šä¼˜å…ˆ event_tsï¼Œç¼ºå¤±æ—¶ç”¨ minute_ts"""
    et = G.nodes[nid].get("event_ts")
    mt = G.nodes[nid].get("minute_ts")
    if et is not None and str(et) != "NaT":
        return pd.Timestamp(et)
    if mt is not None and str(mt) != "NaT":
        return pd.Timestamp(mt)
    return None

def _node_z(G, nid):
    """è·å–èŠ‚ç‚¹å¼‚å¸¸å¼ºåº¦ï¼šäº‹ä»¶èŠ‚ç‚¹ä¸Šé€šå¸¸æœ‰ zscoreï¼›æœåŠ¡èŠ‚ç‚¹å¯èƒ½æ— """
    z = G.nodes[nid].get("zscore")
    try:
        return float(z) if z is not None else 0.0
    except:
        return 0.0
```

### 2. **èŠ‚ç‚¹/ç±»å‹å¤§å°å†™ä¸ä¸€è‡´ä¿®æ­£**
**é—®é¢˜**: å¯¼å‡º/Loader é‡Œæ˜¯ `type="service"|"metric_event"|"log_event"`ï¼Œä½† `_score_root_causes` åˆ¤æ–­ `node_type == 'Service'`ã€‚

**ä¿®æ­£**: ç»Ÿä¸€ç”¨å°å†™æšä¸¾ `"service"`ï¼š
```python
if n.get("type") == "service":  # ç»Ÿä¸€å°å†™
```

### 3. **è§„åˆ™åç½®çš„ key ä¸åŒ¹é…ä¿®æ­£**
**é—®é¢˜**: CMRW è§„åˆ’äº† `rule_bias[(src_type, edge_type, dst_type)]`ï¼Œä½† `_rules_to_bias` è¾“å‡ºçš„æ˜¯"ä»… edge-type åºåˆ—"çš„å…ƒç»„ã€‚

**ä¿®æ­£**: æ”¹æˆäº§å‡º `(src_type, edge_type, dst_type) -> weight`ï¼š
```python
def _learn_rules_from_paths(paths: List[List[str]], G: nx.MultiDiGraph) -> List[Dict[str, Any]]:
    """ä»è·¯å¾„æŠ½å–ï¼š(src_type, edge_type, dst_type) æ¨¡å¼åŠå…¶å‡ºç°ç»Ÿè®¡"""
    stats = {}
    for path in paths:
        for i in range(len(path) - 1):
            u, v = path[i], path[i+1]
            u_type = G.nodes[u].get("type", "unknown")
            v_type = G.nodes[v].get("type", "unknown")
            edict = G.get_edge_data(u, v) or {}
            for _, edata in edict.items():
                etype = edata.get("type", "unknown")
                key = (u_type, etype, v_type)  # ä¸‰å…ƒç»„æ ¼å¼
                stats.setdefault(key, {"support": 0, "examples": 0})
                stats[key]["support"] += 1
                break

def _rules_to_bias(rules: List[Dict[str, Any]], cap: float = 1.3) -> Dict[tuple, float]:
    """è¾“å‡ºç»™ WalkConfig.rule_bias: {(src_type, edge_type, dst_type): weight}"""
    bias = {}
    if not rules:
        return bias
    max_supp = max(r["support"] for r in rules) or 1
    for r in rules:
        s = r["support"] / max_supp
        c = r["confidence"]
        w = 1.0 + (cap - 1.0) * 0.5 * (s + c)
        bias[r["triple"]] = float(min(max(w, 0.7), cap))
    return bias
```

### 4. **æ·»åŠ "æ»šåŠ¨çª—å£"æœºåˆ¶**
**é—®é¢˜**: åªèµ°äº†ä¸€è½®ï¼Œæ²¡æœ‰"é‡‡æ ·â†’è§„åˆ™â†’åç½®â†’ä¸‹ä¸€çª—â†’â€¦ æ”¶æ•›"ã€‚

**ä¿®æ­£**: åŠ å°å¾ªç¯ï¼ŒæŒ‰è§„åˆ™æ›´æ–° `WalkConfig.rule_bias`ï¼Œå¹¶æŠŠ `next_center_ts` è®¾ä¸ºè¯æ®é“¾**æœ€æ—©äº‹ä»¶**åˆ†é’Ÿï¼š
```python
def run_llmda_rca(index_paths: dict, top_info_id: str, init_center_ts: str, k_minutes: int = 5,
                  max_rounds: int = 4, converge_eps: float = 1e-3) -> dict:
    loader = TKGLoader(index_paths['nodes_path'], index_paths['edges_path'], index_paths['index_path'])
    center_ts = init_center_ts
    last_best = None
    accumulated_bias = {}  # ç´¯ç§¯è§„åˆ™åç½®

    for rd in range(max_rounds):
        G_win = loader.load_window_graph(center_ts, k_minutes)
        
        cfg = WalkConfig(
            max_len=4, num_paths=200, time_monotonic=True,
            allowed_edge_types=("precedes","has_log","has_metric","calls","depends_on"),
            base_weights={"precedes":1.0,"has_metric":1.0,"has_log":1.0,"calls":0.6,"depends_on":0.6},
            rule_bias=accumulated_bias or None
        )

        paths = temporal_random_walk(G_win, [top_info_id], cfg, save_dir="sampled_path", center_ts_iso=center_ts)
        if not paths:
            break

        rules = _learn_rules_from_paths(paths, G_win)
        ranked = rules  # å·²æ’åº
        bias = _rules_to_bias(ranked, cap=1.3)
        # ç´¯ç§¯åç½®ï¼ˆæ¸©å’Œå åŠ ï¼‰
        for k, w in bias.items():
            accumulated_bias[k] = float(min(1.5, accumulated_bias.get(k, 1.0) * (0.7 + 0.3*w)))

        root_causes = _score_root_causes(paths, G_win)
        best = _select_best_root_cause(root_causes)

        # æ”¶æ•›åˆ¤å®š
        if last_best and abs(best["confidence"] - last_best["confidence"]) < converge_eps:
            break
        last_best = best

        # ä¸‹ä¸€çª—å£ï¼šå–"è¯æ®ä¸­æœ€æ—©äº‹ä»¶çš„åˆ†é’Ÿ"
        all_times = []
        for p in paths:
            for nid in p:
                t = _node_time(G_win, nid)
                if t is not None:
                    all_times.append(pd.Timestamp(t))
        if all_times:
            earliest = min(all_times)
            center_ts = earliest.isoformat()
        else:
            break
```

### 5. **è¡¥é½ç¼ºå¤±çš„ import**
**é—®é¢˜**: æœ¬æ–‡ä»¶å†…ç”¨åˆ°äº† `np/re/shutil` ç­‰ä½†æ²¡å¯¼å…¥ã€‚

**ä¿®æ­£**: æ·»åŠ ç¼ºå¤±çš„å¯¼å…¥ï¼š
```python
import numpy as np
import re
import shutil
import json
import pandas as pd
import networkx as nx
from datetime import datetime
```

### 6. **ç©ºè·¯å¾„/ç¼ºèµ·ç‚¹çš„å…œåº•**
**é—®é¢˜**: `start_node` ä¸åœ¨å›¾ã€æˆ–é‡‡æ · 0 æ¡è·¯å¾„æ—¶æ²¡æœ‰ fallbackã€‚

**ä¿®æ­£**: æ·»åŠ å…œåº•æœºåˆ¶ï¼š
```python
if not paths:
    # å…œåº•ï¼šç›´æ¥ç»™å‡ºç©ºç»“æœ
    break
```

### 7. **ç»“æœè½ç›˜æ–‡ä»¶åä¿®æ­£**
**é—®é¢˜**: `_save_rca_result(result, top_info_id)` ç›´æ¥æ‹¿ `top_info_id` åšæ–‡ä»¶åï¼Œé‡Œé¢å¸¸å«å†’å·ç­‰éæ³•å­—ç¬¦ã€‚

**ä¿®æ­£**: åš sanitizeï¼š
```python
def _safe_name(s: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return re.sub(r"[^A-Za-z0-9._-]+", "_", s)

# åœ¨è°ƒç”¨æ—¶ä½¿ç”¨
_save_rca_result(result, _safe_name(top_info_id))
```

## ğŸ“Š æµ‹è¯•ç»“æœéªŒè¯

### è¾…åŠ©å‡½æ•°æµ‹è¯•
```
âœ… _node_time: 2021-03-04 14:31:15.123000+00:00
âœ… _node_z: 2.5
âœ… _safe_name: test_node_with_colons
```

### LLM-DA RCA åŠŸèƒ½æµ‹è¯•
```
âœ… TKG Loader ready: nodes=5, edges=6
âœ… precedes monotonic: 2/2
âœ… Graph built: |V|=5, |E|=6
âœ… saved 1 paths -> sampled_path/2021-03-04T14-31-30Z/paths_1.jsonl
âœ… RCA ç»“æœå·²ä¿å­˜åˆ°: outputs/rca_runs/met_payment_CPU_2021-03-04T14_31_15.123Z_result.json

âœ… è§„åˆ™æ ¼å¼æ­£ç¡®: ["('metric_event', 'precedes', 'metric_event')", "('metric_event', 'precedes', 'log_event')"]
âœ… è¯æ®è·¯å¾„ç”ŸæˆæˆåŠŸ: 1 æ¡
âœ… ç»“æœæ–‡ä»¶ä¿å­˜æˆåŠŸ: outputs/rca_runs/met_payment_CPU_2021-03-04T14_31_15.123Z_result.json
```

## ğŸ”§ æ ¸å¿ƒä¿®æ­£ç‚¹

### 1. **ç»Ÿä¸€çš„æ—¶é—´å¤„ç†**
```python
def _node_time(G, nid):
    """è·å–èŠ‚ç‚¹æ—¶é—´ï¼šä¼˜å…ˆ event_tsï¼Œç¼ºå¤±æ—¶ç”¨ minute_ts"""
    et = G.nodes[nid].get("event_ts")
    mt = G.nodes[nid].get("minute_ts")
    if et is not None and str(et) != "NaT":
        return pd.Timestamp(et)
    if mt is not None and str(mt) != "NaT":
        return pd.Timestamp(mt)
    return None
```

### 2. **æ­£ç¡®çš„è§„åˆ™å­¦ä¹ **
```python
def _learn_rules_from_paths(paths: List[List[str]], G: nx.MultiDiGraph) -> List[Dict[str, Any]]:
    """ä»è·¯å¾„æŠ½å–ï¼š(src_type, edge_type, dst_type) æ¨¡å¼åŠå…¶å‡ºç°ç»Ÿè®¡"""
    stats = {}
    for path in paths:
        for i in range(len(path) - 1):
            u, v = path[i], path[i+1]
            u_type = G.nodes[u].get("type", "unknown")
            v_type = G.nodes[v].get("type", "unknown")
            edict = G.get_edge_data(u, v) or {}
            for _, edata in edict.items():
                etype = edata.get("type", "unknown")
                key = (u_type, etype, v_type)  # ä¸‰å…ƒç»„æ ¼å¼
                stats.setdefault(key, {"support": 0, "examples": 0})
                stats[key]["support"] += 1
                break
```

### 3. **æ­£ç¡®çš„è§„åˆ™åç½®**
```python
def _rules_to_bias(rules: List[Dict[str, Any]], cap: float = 1.3) -> Dict[tuple, float]:
    """è¾“å‡ºç»™ WalkConfig.rule_bias: {(src_type, edge_type, dst_type): weight}"""
    bias = {}
    if not rules:
        return bias
    max_supp = max(r["support"] for r in rules) or 1
    for r in rules:
        s = r["support"] / max_supp
        c = r["confidence"]
        w = 1.0 + (cap - 1.0) * 0.5 * (s + c)
        bias[r["triple"]] = float(min(max(w, 0.7), cap))
    return bias
```

### 4. **æ»šåŠ¨çª—å£æœºåˆ¶**
```python
for rd in range(max_rounds):
    # åŠ è½½æ—¶é—´çª—å£å›¾
    G_win = loader.load_window_graph(center_ts, k_minutes)
    
    # æ‰§è¡Œå—çº¦æŸéšæœºæ¸¸èµ°ï¼ˆå¸¦ç´¯ç§¯åç½®ï¼‰
    cfg = WalkConfig(rule_bias=accumulated_bias or None)
    paths = temporal_random_walk(G_win, [top_info_id], cfg, save_dir="sampled_path", center_ts_iso=center_ts)
    
    # å­¦ä¹ è§„åˆ™å¹¶ç´¯ç§¯åç½®
    rules = _learn_rules_from_paths(paths, G_win)
    bias = _rules_to_bias(rules, cap=1.3)
    for k, w in bias.items():
        accumulated_bias[k] = float(min(1.5, accumulated_bias.get(k, 1.0) * (0.7 + 0.3*w)))
    
    # æ”¶æ•›åˆ¤å®š
    if last_best and abs(best["confidence"] - last_best["confidence"]) < converge_eps:
        break
    
    # ä¸‹ä¸€çª—å£ï¼šå–"è¯æ®ä¸­æœ€æ—©äº‹ä»¶çš„åˆ†é’Ÿ"
    all_times = []
    for p in paths:
        for nid in p:
            t = _node_time(G_win, nid)
            if t is not None:
                all_times.append(pd.Timestamp(t))
    if all_times:
        earliest = min(all_times)
        center_ts = earliest.isoformat()
    else:
        break
```

### 5. **å®‰å…¨çš„æ–‡ä»¶åå¤„ç†**
```python
def _safe_name(s: str) -> str:
    """æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦"""
    return re.sub(r"[^A-Za-z0-9._-]+", "_", s)
```

## ğŸ¯ å¯¹åç»­åˆ†æçš„å½±å“

### 1. **æ—¶é—´è¯­ä¹‰å‡†ç¡®æ€§**
- ç»Ÿä¸€çš„ `event_ts/minute_ts` å¤„ç†ç¡®ä¿æ—¶é—´çº¦æŸå‡†ç¡®
- æ­£ç¡®çš„å¼‚å¸¸å¼ºåº¦ `zscore` æ”¯æŒå‡†ç¡®çš„æ ¹å› æ‰“åˆ†

### 2. **è§„åˆ™å­¦ä¹ è´¨é‡**
- æ­£ç¡®çš„ä¸‰å…ƒç»„æ ¼å¼ `(src_type, edge_type, dst_type)` æ”¯æŒ CMRW åç½®
- ç´¯ç§¯åç½®æœºåˆ¶æ”¯æŒè¿­ä»£æ”¶æ•›

### 3. **æ»šåŠ¨åˆ†æèƒ½åŠ›**
- å¤šè½®è¿­ä»£æ”¯æŒ"é‡‡æ ·â†’è§„åˆ™â†’åç½®â†’ä¸‹ä¸€çª—â†’â€¦ æ”¶æ•›"
- æœ€æ—©äº‹ä»¶æ—¶é—´é€‰æ‹©æ”¯æŒæ—¶é—´å›æº¯åˆ†æ

### 4. **ç³»ç»Ÿç¨³å®šæ€§**
- å®Œæ•´çš„å¯¼å…¥å’Œå…œåº•æœºåˆ¶ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œ
- å®‰å…¨çš„æ–‡ä»¶åå¤„ç†é¿å…æ–‡ä»¶ç³»ç»Ÿé”™è¯¯

## ğŸš€ ä½¿ç”¨å»ºè®®

### 1. **é…ç½®å‚æ•°**
```python
result = run_llmda_rca(
    index_paths=index_paths,
    top_info_id="met:payment:CPU:2021-03-04T14:31:15.123Z",
    init_center_ts="2021-03-04T14:31:30Z",
    k_minutes=5,
    max_rounds=4,
    converge_eps=1e-3
)
```

### 2. **ç»“æœåˆ†æ**
```python
# æ£€æŸ¥æ ¹å› è¯†åˆ«ç»“æœ
print(f"æ ¹å› æœåŠ¡: {result['root_service']}")
print(f"æ ¹å› åŸå› : {result['root_reason']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']}")

# æ£€æŸ¥ä½¿ç”¨çš„è§„åˆ™
print(f"ä½¿ç”¨è§„åˆ™: {result['rules_used']}")

# æ£€æŸ¥è¯æ®è·¯å¾„
print(f"è¯æ®è·¯å¾„: {result['evidence_paths']}")
```

### 3. **è¾“å‡ºæ–‡ä»¶**
- è·¯å¾„ä¿å­˜åœ¨ `sampled_path/{center_ts}/paths_*.jsonl`
- ç»“æœä¿å­˜åœ¨ `outputs/rca_runs/{sanitized_case_id}_result.json`

## ğŸ“ æ€»ç»“

ä¿®æ­£åçš„ LLM-DA RCA è§£å†³äº†æ‰€æœ‰å…³é”®é—®é¢˜ï¼š

1. âœ… **æ—¶é—´å­—æ®µç»Ÿä¸€**: ä½¿ç”¨ `event_ts/minute_ts` å’Œ `zscore`
2. âœ… **èŠ‚ç‚¹ç±»å‹ç»Ÿä¸€**: ä½¿ç”¨å°å†™æšä¸¾ `"service"`
3. âœ… **è§„åˆ™åç½®åŒ¹é…**: è¾“å‡º `(src_type, edge_type, dst_type)` æ ¼å¼
4. âœ… **æ»šåŠ¨çª—å£æœºåˆ¶**: æ”¯æŒå¤šè½®è¿­ä»£å’Œåç½®ç´¯ç§¯
5. âœ… **å¯¼å…¥å®Œæ•´æ€§**: è¡¥é½æ‰€æœ‰ç¼ºå¤±çš„å¯¼å…¥
6. âœ… **å…œåº•æœºåˆ¶**: å¤„ç†ç©ºè·¯å¾„å’Œç¼ºèµ·ç‚¹æƒ…å†µ
7. âœ… **æ–‡ä»¶åå®‰å…¨**: æ¸…ç†éæ³•å­—ç¬¦é¿å…æ–‡ä»¶ç³»ç»Ÿé”™è¯¯

è¿™äº›ä¿®æ­£ç¡®ä¿äº† LLM-DA RCA èƒ½å¤Ÿæ­£ç¡®åœ°è¿›è¡Œ"é‡‡æ ·â†’è§„åˆ™â†’åç½®â†’ä¸‹ä¸€çª—â†’â€¦ æ”¶æ•›"çš„é—­ç¯åˆ†æï¼Œä¸ºåç»­çš„æ ¹å› åˆ†ææä¾›äº†å¯é çš„æ—¶é—´è¯­ä¹‰ã€è§„åˆ™å­¦ä¹ å’Œæ»šåŠ¨åˆ†æèƒ½åŠ›ã€‚

ç°åœ¨çš„ LLM-DA RCA æ˜¯ä¸€ä¸ªçœŸæ­£å¯é çš„"æ»šåŠ¨æ ¹å› åˆ†æå™¨"ï¼Œèƒ½å¤Ÿæ­£ç¡®å¤„ç†æ—¶é—´çº¦æŸã€è§„åˆ™å­¦ä¹ ã€åç½®ç´¯ç§¯å’Œè¿­ä»£æ”¶æ•›ï¼Œä¸ºåç»­çš„æ ¹å› åˆ†ææä¾›äº†å®Œæ•´çš„æ•°æ®åŸºç¡€ï¼
