#!/usr/bin/env python3
"""
æµ‹è¯•æ‰¹é‡ RCA åˆ†æåŠŸèƒ½
"""
import os
import sys
import json
import pandas as pd
import networkx as nx
from pathlib import Path

# Add project paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# Add LLM-DA path
LLM_DA_ROOT = os.path.abspath(os.path.join(PROJECT_ROOT, "LLM-DA"))
if LLM_DA_ROOT not in sys.path:
    sys.path.insert(0, LLM_DA_ROOT)

def create_batch_test_data():
    """åˆ›å»ºæ‰¹é‡æµ‹è¯•æ•°æ®"""
    print("ğŸ“Š åˆ›å»ºæ‰¹é‡æµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dirs = [
        "data/Bank",
        "outputs/Bank",
        "LLM-DA/datasets/tkg/Bank_1",
        "LLM-DA/datasets/tkg/Bank_2"
    ]
    
    for dir_path in test_dirs:
        os.makedirs(dir_path, exist_ok=True)
    
    # åˆ›å»ºæµ‹è¯• query.csvï¼ˆå¤šä¸ªé—®é¢˜ï¼‰
    query_data = [
        {
            "problem_number": 1,
            "instruction": "At 2021-03-04 14:31:00, the payment service experienced high CPU usage and memory spikes, leading to HTTP 500 errors. Please analyze the root cause."
        },
        {
            "problem_number": 2,
            "instruction": "At 2021-03-04 14:35:00, the database service showed connection timeout errors. Please analyze the root cause."
        }
    ]
    
    query_df = pd.DataFrame(query_data)
    query_df.to_csv("data/Bank/query.csv", index=False)
    
    # ä¸ºæ¯ä¸ªé—®é¢˜åˆ›å»º GraphML æ–‡ä»¶
    for problem_num in [1, 2]:
        G = nx.MultiDiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹
        G.add_node("svc:payment", type="service", service="payment", minute_ts="2021-03-04T14:31:00Z")
        G.add_node("svc:database", type="service", service="database", minute_ts="2021-03-04T14:31:00Z")
        G.add_node(f"met:payment:CPU:2021-03-04T14:31:15.123Z", type="metric_event", service="payment", 
                   metric="CPU", zscore=3.5, value=95.2, minute_ts="2021-03-04T14:31:00Z", 
                   event_ts="2021-03-04T14:31:15.123Z")
        G.add_node(f"met:payment:Memory:2021-03-04T14:31:45.456Z", type="metric_event", service="payment", 
                   metric="Memory", zscore=2.8, value=88.5, minute_ts="2021-03-04T14:31:00Z", 
                   event_ts="2021-03-04T14:31:45.456Z")
        G.add_node(f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", type="log_event", service="payment", 
                   template_id="HTTP_500_ERROR", severity="ERROR", minute_ts="2021-03-04T14:32:00Z", 
                   event_ts="2021-03-04T14:32:10.789Z")
        
        # æ·»åŠ è¾¹
        G.add_edge("svc:payment", f"met:payment:CPU:2021-03-04T14:31:15.123Z", type="has_metric", weight=1.0, minute_ts="2021-03-04T14:31:00Z")
        G.add_edge("svc:payment", f"met:payment:Memory:2021-03-04T14:31:45.456Z", type="has_metric", weight=1.0, minute_ts="2021-03-04T14:31:00Z")
        G.add_edge("svc:payment", f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", type="has_log", weight=1.0, minute_ts="2021-03-04T14:32:00Z")
        G.add_edge(f"met:payment:CPU:2021-03-04T14:31:15.123Z", f"met:payment:Memory:2021-03-04T14:31:45.456Z", type="precedes", weight=1.0, minute_ts="2021-03-04T14:31:00Z")
        G.add_edge(f"met:payment:Memory:2021-03-04T14:31:45.456Z", f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", type="precedes", weight=1.0, minute_ts="2021-03-04T14:32:00Z")
        G.add_edge("svc:payment", "svc:database", type="calls", weight=0.8, minute_ts="2021-03-04T14:31:00Z")
        
        # ä¿å­˜ GraphML æ–‡ä»¶
        graphml_path = f"outputs/Bank/Bank_{problem_num}.graphml"
        nx.write_graphml(G, graphml_path)
        print(f"âœ… åˆ›å»º GraphML æ–‡ä»¶: {graphml_path}")
        
        # åˆ›å»ºå¯¹åº”çš„ TKG æ•°æ®
        nodes_data = [
            {"id": "svc:payment", "node_type": "service", "service": "payment", 
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": None},
            {"id": "svc:database", "node_type": "service", "service": "database", 
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": None},
            {"id": f"met:payment:CPU:2021-03-04T14:31:15.123Z", "node_type": "metric_event", 
             "service": "payment", "metric": "CPU", "zscore": 3.5, "value": 95.2,
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": "2021-03-04T14:31:15.123Z"},
            {"id": f"met:payment:Memory:2021-03-04T14:31:45.456Z", "node_type": "metric_event", 
             "service": "payment", "metric": "Memory", "zscore": 2.8, "value": 88.5,
             "minute_ts": "2021-03-04T14:31:00Z", "event_ts": "2021-03-04T14:31:45.456Z"},
            {"id": f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", "node_type": "log_event", 
             "service": "payment", "template_id": "HTTP_500_ERROR", "severity": "ERROR",
             "minute_ts": "2021-03-04T14:32:00Z", "event_ts": "2021-03-04T14:32:10.789Z"},
        ]
        
        nodes_df = pd.DataFrame(nodes_data)
        nodes_df.to_parquet(f"LLM-DA/datasets/tkg/Bank_{problem_num}/nodes.parquet", index=False)
        
        # åˆ›å»º edges.parquet
        edges_data = [
            {"src": "svc:payment", "dst": f"met:payment:CPU:2021-03-04T14:31:15.123Z", 
             "edge_type": "has_metric", "weight": 1.0, "minute_ts": "2021-03-04T14:31:00Z"},
            {"src": "svc:payment", "dst": f"met:payment:Memory:2021-03-04T14:31:45.456Z", 
             "edge_type": "has_metric", "weight": 1.0, "minute_ts": "2021-03-04T14:31:00Z"},
            {"src": "svc:payment", "dst": f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", 
             "edge_type": "has_log", "weight": 1.0, "minute_ts": "2021-03-04T14:32:00Z"},
            {"src": f"met:payment:CPU:2021-03-04T14:31:15.123Z", 
             "dst": f"met:payment:Memory:2021-03-04T14:31:45.456Z", 
             "edge_type": "precedes", "weight": 1.0, "minute_ts": "2021-03-04T14:31:00Z"},
            {"src": f"met:payment:Memory:2021-03-04T14:31:45.456Z", 
             "dst": f"log:payment:HTTP_500_ERROR:2021-03-04T14:32:10.789Z", 
             "edge_type": "precedes", "weight": 1.0, "minute_ts": "2021-03-04T14:32:00Z"},
            {"src": "svc:payment", "dst": "svc:database", 
             "edge_type": "calls", "weight": 0.8, "minute_ts": "2021-03-04T14:31:00Z"},
        ]
        
        edges_df = pd.DataFrame(edges_data)
        edges_df.to_parquet(f"LLM-DA/datasets/tkg/Bank_{problem_num}/edges.parquet", index=False)
        
        # åˆ›å»º index.json
        index_data = {
            "time_range": {
                "start": 1614868260.0,
                "end": 1614868320.0
            },
            "minutes": [
                {
                    "time_str": "14-31-00",
                    "minute_ts": 1614868260.0,
                    "minute_dt": "2021-03-04T14:31:00Z",
                    "nodes_count": 4,
                    "edges_count": 3,
                    "graph_path": f"Bank_{problem_num}.graphml"
                },
                {
                    "time_str": "14-32-00", 
                    "minute_ts": 1614868320.0,
                    "minute_dt": "2021-03-04T14:32:00Z",
                    "nodes_count": 1,
                    "edges_count": 2,
                    "graph_path": f"Bank_{problem_num}.graphml"
                }
            ],
            "total_nodes": 5,
            "total_edges": 6
        }
        
        with open(f"LLM-DA/datasets/tkg/Bank_{problem_num}/index.json", 'w', encoding='utf-8') as f:
            json.dump(index_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… æ‰¹é‡æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ")
    return True

def test_batch_analysis():
    """æµ‹è¯•æ‰¹é‡åˆ†æåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ‰¹é‡åˆ†æåŠŸèƒ½")
    print("=" * 50)
    
    try:
        from agent.rca_llmda_agent import RCALLMDAgent
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        create_batch_test_data()
        
        # åˆ›å»º Agent
        agent = RCALLMDAgent()
        
        print(f"ğŸ“Š æµ‹è¯•å‚æ•°:")
        print(f"   æ•°æ®é›†: Bank")
        print(f"   æœ€å¤§é—®é¢˜æ•°: 2")
        
        # æµ‹è¯•æ‰¹é‡åˆ†æ
        print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ...")
        results = agent.batch_analysis("Bank", max_problems=2)
        
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æç»“æœ:")
        print(f"   æ€»é—®é¢˜æ•°: {len(results)}")
        
        successful = sum(1 for r in results if r.get('status') != 'failed')
        failed = len(results) - successful
        
        print(f"   æˆåŠŸæ•°: {successful}")
        print(f"   å¤±è´¥æ•°: {failed}")
        
        # è¯¦ç»†ç»“æœ
        for i, result in enumerate(results, 1):
            print(f"\n   é—®é¢˜ {i}:")
            print(f"     çŠ¶æ€: {result.get('status', 'unknown')}")
            if result.get('status') == 'success':
                print(f"     æ ¹å› æœåŠ¡: {result.get('root_service', 'unknown')}")
                print(f"     æ ¹å› åŸå› : {result.get('root_reason', 'unknown')}")
                print(f"     ç½®ä¿¡åº¦: {result.get('confidence', 0.0):.3f}")
            else:
                print(f"     é”™è¯¯: {result.get('error', 'unknown error')}")
        
        # éªŒè¯æ‰¹é‡ç»“æœ
        if successful > 0:
            print(f"\n   âœ… æ‰¹é‡åˆ†æéƒ¨åˆ†æˆåŠŸ")
        else:
            print(f"\n   âš ï¸ æ‰¹é‡åˆ†æå…¨éƒ¨å¤±è´¥")
            return False
        
        # æ£€æŸ¥æ‰¹é‡ç»“æœæ–‡ä»¶
        batch_result_file = "outputs/rca_analysis/Bank/batch_results.json"
        if os.path.exists(batch_result_file):
            print(f"\nğŸ“¤ æ‰¹é‡ç»“æœæ–‡ä»¶ä¿å­˜æˆåŠŸ: {batch_result_file}")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            with open(batch_result_file, 'r', encoding='utf-8') as f:
                saved_results = json.load(f)
            
            if len(saved_results) == 2:
                print(f"   âœ… æ‰¹é‡ç»“æœæ–‡ä»¶åŒ…å«æ­£ç¡®çš„é—®é¢˜æ•°é‡")
            else:
                print(f"   âš ï¸ æ‰¹é‡ç»“æœæ–‡ä»¶é—®é¢˜æ•°é‡ä¸æ­£ç¡®")
        else:
            print(f"\n   âŒ æ‰¹é‡ç»“æœæ–‡ä»¶ä¿å­˜å¤±è´¥")
            return False
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def cleanup_test_data():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    import shutil
    
    test_dirs = [
        "data/Bank",
        "outputs/Bank",
        "LLM-DA/datasets/tkg/Bank_1",
        "LLM-DA/datasets/tkg/Bank_2",
        "outputs/rca_analysis"
    ]
    
    for dir_path in test_dirs:
        if os.path.exists(dir_path):
            shutil.rmtree(dir_path)
            print(f"ğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®: {dir_path}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ‰¹é‡ RCA åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 70)
    
    try:
        # æµ‹è¯•æ‰¹é‡åˆ†æ
        batch_success = test_batch_analysis()
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cleanup_test_data()
        
        # è¾“å‡ºç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   æ‰¹é‡åˆ†æ: {'âœ… é€šè¿‡' if batch_success else 'âŒ å¤±è´¥'}")
        
        print(f"\nğŸ“ˆ æ€»ä½“ç»“æœ: {'âœ… é€šè¿‡' if batch_success else 'âŒ å¤±è´¥'}")
        
        return batch_success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        cleanup_test_data()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
