#!/usr/bin/env python3
"""
RCA LLM-DA Agent - KG-RCA2 ä¸ LLM-DA é›†æˆå…¥å£
è°ƒç”¨ run_llmda_rca å®Œæˆæ•´æ¡ˆ RCA åˆ†æ
"""
import os
import sys
import json
import yaml
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

# Add project paths
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# Add LLM-DA path
LLM_DA_ROOT = os.path.abspath(os.path.join(PROJECT_ROOT, "LLM-DA"))
if LLM_DA_ROOT not in sys.path:
    sys.path.insert(0, LLM_DA_ROOT)

from kg_rca.adapters.tkg_export import export_tkg_slices
from kg_rca.timeutil import extract_and_convert_datetime


class RCALLMDAgent:
    """
    RCA LLM-DA Agent for integrated root cause analysis
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Initialize RCA LLM-DA Agent
        
        Args:
            config_path: Path to configuration file
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # LLM-DA integration paths
        self.llmda_config = self.config.get('llmda', {})
        self.nodes_path = self.llmda_config.get('nodes_path', 'LLM-DA/datasets/tkg/nodes.parquet')
        self.edges_path = self.llmda_config.get('edges_path', 'LLM-DA/datasets/tkg/edges.parquet')
        self.index_path = self.llmda_config.get('index_path', 'LLM-DA/datasets/tkg/index.json')
        self.k_minutes = self.llmda_config.get('k_minutes', 5)
        
        print(f"ğŸ¤– RCA LLM-DA Agent åˆå§‹åŒ–å®Œæˆ")
        print(f"   é…ç½®æ–‡ä»¶: {config_path}")
        print(f"   æ•°æ®è·¯å¾„: {self.nodes_path}")
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        else:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            return {}
    
    def run_rca_analysis(self, dataset: str, problem_number: int = 1) -> Dict[str, Any]:
        """
        è¿è¡Œ RCA åˆ†æ
        
        Args:
            dataset: Dataset name (Bank, Telecom, etc.)
            problem_number: Problem number to analyze
            
        Returns:
            RCA analysis results
        """
        print(f"ğŸ”„ å¼€å§‹ RCA åˆ†æ")
        print(f"   æ•°æ®é›†: {dataset}")
        print(f"   é—®é¢˜ç¼–å·: {problem_number}")
        
        # 1. å¯¼å‡º TKG æ•°æ®
        print("ğŸ“Š æ­¥éª¤ 1: å¯¼å‡º TKG æ•°æ®...")
        tkg_result = self._export_tkg_data(dataset, problem_number)
        
        if not tkg_result['nodes_path']:
            return {
                'error': 'Failed to export TKG data',
                'status': 'failed'
            }
        
        # 2. è¿è¡Œ LLM-DA RCA
        print("ğŸ§  æ­¥éª¤ 2: è¿è¡Œ LLM-DA RCA...")
        rca_result = self._run_llmda_rca(tkg_result, dataset, problem_number)
        
        # 3. ä¿å­˜ç»“æœ
        print("ğŸ’¾ æ­¥éª¤ 3: ä¿å­˜ç»“æœ...")
        self._save_analysis_result(rca_result, dataset, problem_number)
        
        return rca_result
    
    def _export_tkg_data(self, dataset: str, problem_number: int) -> Dict[str, Any]:
        """
        å¯¼å‡º TKG æ•°æ®ï¼ˆä¼˜åŒ–ï¼šé¿å…é‡å¤å¯¼å‡ºï¼ŒæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰
        
        Args:
            dataset: Dataset name
            problem_number: Problem number
            
        Returns:
            Export result dictionary
        """
        # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæŒ‰ dataset/problem_number åˆ†ç›®å½•ï¼‰
        output_dir = f"outputs/{dataset}"
        merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”æ–°äºæºæ–‡ä»¶
        nodes_path = os.path.join(merged_dir, "nodes.parquet")
        edges_path = os.path.join(merged_dir, "edges.parquet")
        index_path = os.path.join(merged_dir, "index.json")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°å¯¼å‡º
        need_export = True
        if all(os.path.exists(p) for p in [nodes_path, edges_path, index_path]):
            # æ£€æŸ¥æºæ–‡ä»¶æ—¶é—´
            source_graphml = os.path.join(output_dir, f"{dataset}_{problem_number}.graphml")
            if os.path.exists(source_graphml):
                source_mtime = os.path.getmtime(source_graphml)
                target_mtime = min(os.path.getmtime(p) for p in [nodes_path, edges_path, index_path])
                if target_mtime > source_mtime:
                    need_export = False
                    print(f"âœ… TKG æ•°æ®å·²å­˜åœ¨ä¸”æœ€æ–°ï¼Œè·³è¿‡å¯¼å‡º")
        
        if need_export:
            # å¯¼å‡º TKG åˆ‡ç‰‡
            result = export_tkg_slices(output_dir, merged_dir)
        else:
            # è¿”å›ç°æœ‰æ–‡ä»¶è·¯å¾„
            result = {
                'nodes_path': nodes_path,
                'edges_path': edges_path,
                'index_path': index_path
            }
        
        if result.get('nodes_path'):
            print(f"âœ… TKG æ•°æ®å‡†å¤‡å®Œæˆ")
            print(f"   èŠ‚ç‚¹æ–‡ä»¶: {result['nodes_path']}")
            print(f"   è¾¹æ–‡ä»¶: {result['edges_path']}")
            print(f"   ç´¢å¼•æ–‡ä»¶: {result['index_path']}")
        else:
            print(f"âŒ TKG å¯¼å‡ºå¤±è´¥")
        
        return result
    
    def _run_llmda_rca(self, tkg_result: Dict[str, Any], dataset: str, problem_number: int) -> Dict[str, Any]:
        """
        è¿è¡Œ LLM-DA RCA åˆ†æ
        
        Args:
            tkg_result: TKG export result
            dataset: Dataset name
            problem_number: Problem number
            
        Returns:
            RCA analysis result
        """
        try:
            # å¯¼å…¥ LLM-DA æ¨¡å—
            from Iteration_reasoning import run_llmda_rca
            
            # æ„å»ºç´¢å¼•è·¯å¾„
            index_paths = {
                'nodes_path': tkg_result['nodes_path'],
                'edges_path': tkg_result['edges_path'],
                'index_path': tkg_result['index_path']
            }
            
            # ç¡®å®šèµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´
            top_info_id, init_center_ts = self._determine_start_node_and_time(dataset, problem_number)
            
            if not top_info_id:
                return {
                    'error': 'Failed to determine start node',
                    'status': 'failed'
                }
            
            print(f"ğŸ¯ èµ·å§‹èŠ‚ç‚¹: {top_info_id}")
            print(f"ğŸ¯ ä¸­å¿ƒæ—¶é—´: {init_center_ts}")
            
            # è¿è¡Œ LLM-DA RCA
            result = run_llmda_rca(
                index_paths=index_paths,
                top_info_id=top_info_id,
                init_center_ts=init_center_ts,
                k_minutes=self.k_minutes
            )
            
            # ç¡®ä¿è¿”å›ç»“æœåŒ…å« status å­—æ®µ
            if 'status' not in result:
                result['status'] = 'success'
            
            return result
            
        except Exception as e:
            print(f"âŒ LLM-DA RCA è¿è¡Œå¤±è´¥: {e}")
            return {
                'error': str(e),
                'status': 'failed'
            }
    
    def _determine_start_node_and_time(self, dataset: str, problem_number: int) -> tuple:
        """
        ç¡®å®šèµ·å§‹èŠ‚ç‚¹å’Œæ—¶é—´ï¼ˆæ™ºèƒ½é€‰æ‹©ï¼šåœ¨æ—¶é—´çª—å£å†…æ‰¾ zscore æœ€å¤§çš„ metric_eventï¼‰
        
        Args:
            dataset: Dataset name
            problem_number: Problem number
            
        Returns:
            Tuple of (start_node_id, center_timestamp)
        """
        # è¯»å–æŸ¥è¯¢æ–‡ä»¶
        query_file = f"data/{dataset}/query.csv"
        
        if not os.path.exists(query_file):
            print(f"âš ï¸ æŸ¥è¯¢æ–‡ä»¶ä¸å­˜åœ¨: {query_file}")
            return None, None
        
        try:
            df = pd.read_csv(query_file)
            
            if problem_number > len(df):
                print(f"âš ï¸ é—®é¢˜ç¼–å· {problem_number} è¶…å‡ºèŒƒå›´")
                return None, None
            
            # è·å–é—®é¢˜æè¿°
            instruction = df.iloc[problem_number - 1]['instruction']
            
            # æå–æ—¶é—´ä¿¡æ¯
            time_dict = extract_and_convert_datetime(instruction)
            center_ts = None
            
            if time_dict and isinstance(time_dict, dict):
                center_ts = time_dict.get('formatted_date')
            
            # å¦‚æœæ— æ³•ä» instruction æå–æ—¶é—´ï¼Œä½¿ç”¨ index.json çš„ä¸­ä½åˆ†é’Ÿ
            if not center_ts:
                center_ts = self._get_median_time_from_index(dataset, problem_number)
                if not center_ts:
                    print(f"âš ï¸ æ— æ³•ç¡®å®šä¸­å¿ƒæ—¶é—´")
                    return None, None
            
            # æ™ºèƒ½é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼šåœ¨å¯¼å‡ºçš„ nodes.parquet ä¸­æ‰¾ zscore æœ€å¤§çš„ metric_event
            start_node_id = self._find_best_start_node(dataset, problem_number, center_ts)
            
            if not start_node_id:
                print(f"âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„èµ·å§‹èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´")
                # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤çš„å¼‚å¸¸æŒ‡æ ‡äº‹ä»¶
                start_node_id = f"met:payment:CPU:{center_ts}"
            
            return start_node_id, center_ts
            
        except Exception as e:
            print(f"âŒ ç¡®å®šèµ·å§‹èŠ‚ç‚¹å¤±è´¥: {e}")
            return None, None
    
    def _find_best_start_node(self, dataset: str, problem_number: int, center_ts: str) -> Optional[str]:
        """
        åœ¨å¯¼å‡ºçš„ nodes.parquet ä¸­æ‰¾ zscore æœ€å¤§çš„ metric_event ä½œä¸ºèµ·å§‹èŠ‚ç‚¹
        
        Args:
            dataset: Dataset name
            problem_number: Problem number
            center_ts: Center timestamp
            
        Returns:
            Best start node ID or None
        """
        try:
            # æ„å»ºèŠ‚ç‚¹æ–‡ä»¶è·¯å¾„
            merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
            nodes_path = os.path.join(merged_dir, "nodes.parquet")
            
            if not os.path.exists(nodes_path):
                print(f"âš ï¸ èŠ‚ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {nodes_path}")
                return None
            
            # è¯»å–èŠ‚ç‚¹æ•°æ®
            nodes_df = pd.read_parquet(nodes_path)
            
            # è¿‡æ»¤ metric_event èŠ‚ç‚¹
            metric_nodes = nodes_df[nodes_df['node_type'] == 'metric_event'].copy()
            
            if metric_nodes.empty:
                print(f"âš ï¸ æœªæ‰¾åˆ° metric_event èŠ‚ç‚¹")
                return None
            
            # è®¡ç®—æ—¶é—´çª—å£ï¼ˆcenter_ts Â± k_minutesï¼‰
            center_dt = pd.to_datetime(center_ts)
            window_start = center_dt - pd.Timedelta(minutes=self.k_minutes)
            window_end = center_dt + pd.Timedelta(minutes=self.k_minutes)
            
            # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„èŠ‚ç‚¹
            metric_nodes['event_ts'] = pd.to_datetime(metric_nodes['event_ts'], errors='coerce')
            metric_nodes['minute_ts'] = pd.to_datetime(metric_nodes['minute_ts'], errors='coerce')
            
            # ä½¿ç”¨ event_ts æˆ– minute_ts
            metric_nodes['time_ts'] = metric_nodes['event_ts'].fillna(metric_nodes['minute_ts'])
            
            # è¿‡æ»¤æ—¶é—´çª—å£
            window_nodes = metric_nodes[
                (metric_nodes['time_ts'] >= window_start) & 
                (metric_nodes['time_ts'] <= window_end)
            ]
            
            if window_nodes.empty:
                print(f"âš ï¸ æ—¶é—´çª—å£å†…æœªæ‰¾åˆ° metric_event èŠ‚ç‚¹")
                return None
            
            # æ‰¾ zscore æœ€å¤§çš„èŠ‚ç‚¹
            window_nodes['zscore'] = pd.to_numeric(window_nodes['zscore'], errors='coerce').fillna(0)
            best_node = window_nodes.loc[window_nodes['zscore'].idxmax()]
            
            print(f"ğŸ¯ é€‰æ‹©èµ·å§‹èŠ‚ç‚¹: {best_node['id']} (zscore: {best_node['zscore']:.2f})")
            return best_node['id']
            
        except Exception as e:
            print(f"âŒ æŸ¥æ‰¾èµ·å§‹èŠ‚ç‚¹å¤±è´¥: {e}")
            return None
    
    def _get_median_time_from_index(self, dataset: str, problem_number: int) -> Optional[str]:
        """
        ä» index.json è·å–ä¸­ä½åˆ†é’Ÿæ—¶é—´ä½œä¸ºå…œåº•
        
        Args:
            dataset: Dataset name
            problem_number: Problem number
            
        Returns:
            Median timestamp string or None
        """
        try:
            # æ„å»ºç´¢å¼•æ–‡ä»¶è·¯å¾„
            merged_dir = f"LLM-DA/datasets/tkg/{dataset}_{problem_number}"
            index_path = os.path.join(merged_dir, "index.json")
            
            if not os.path.exists(index_path):
                print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
                return None
            
            # è¯»å–ç´¢å¼•æ•°æ®
            with open(index_path, 'r', encoding='utf-8') as f:
                index_data = json.load(f)
            
            # è·å–åˆ†é’Ÿåˆ—è¡¨
            minutes = index_data.get('minutes', [])
            if not minutes:
                print(f"âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸­æ²¡æœ‰åˆ†é’Ÿæ•°æ®")
                return None
            
            # è®¡ç®—ä¸­ä½åˆ†é’Ÿ
            median_idx = len(minutes) // 2
            median_minute = minutes[median_idx]
            center_ts = median_minute.get('minute_dt')
            
            if center_ts:
                print(f"ğŸ¯ ä½¿ç”¨ç´¢å¼•ä¸­ä½æ—¶é—´: {center_ts}")
                return center_ts
            else:
                print(f"âš ï¸ æ— æ³•ä»ç´¢å¼•è·å–æ—¶é—´")
                return None
                
        except Exception as e:
            print(f"âŒ è·å–ä¸­ä½æ—¶é—´å¤±è´¥: {e}")
            return None
    
    def _save_analysis_result(self, result: Dict[str, Any], dataset: str, problem_number: int):
        """
        ä¿å­˜åˆ†æç»“æœ
        
        Args:
            result: Analysis result
            dataset: Dataset name
            problem_number: Problem number
        """
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = f"outputs/rca_analysis/{dataset}"
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(output_dir, f"problem_{problem_number}_result.json")
        
        # æ·»åŠ å…ƒæ•°æ®
        result['metadata'] = {
            'dataset': dataset,
            'problem_number': problem_number,
            'analysis_time': datetime.now().isoformat(),
            'config': self.config
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“¤ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    def batch_analysis(self, dataset: str, max_problems: int = 5) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªé—®é¢˜
        
        Args:
            dataset: Dataset name
            max_problems: Maximum number of problems to analyze
            
        Returns:
            List of analysis results
        """
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ: {dataset} (æœ€å¤š {max_problems} ä¸ªé—®é¢˜)")
        
        results = []
        
        for problem_number in range(1, max_problems + 1):
            print(f"\nğŸ“Š åˆ†æé—®é¢˜ {problem_number}/{max_problems}")
            
            try:
                result = self.run_rca_analysis(dataset, problem_number)
                results.append(result)
                
                if result.get('status') == 'failed':
                    print(f"âŒ é—®é¢˜ {problem_number} åˆ†æå¤±è´¥")
                    continue
                
                print(f"âœ… é—®é¢˜ {problem_number} åˆ†æå®Œæˆ")
                print(f"   æ ¹å› æœåŠ¡: {result.get('root_service', 'unknown')}")
                print(f"   æ ¹å› åŸå› : {result.get('root_reason', 'unknown')}")
                print(f"   ç½®ä¿¡åº¦: {result.get('confidence', 0.0):.3f}")
                
            except Exception as e:
                print(f"âŒ é—®é¢˜ {problem_number} åˆ†æå¼‚å¸¸: {e}")
                results.append({
                    'problem_number': problem_number,
                    'error': str(e),
                    'status': 'failed'
                })
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        batch_result_file = f"outputs/rca_analysis/{dataset}/batch_results.json"
        with open(batch_result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“¤ æ‰¹é‡åˆ†æç»“æœå·²ä¿å­˜åˆ°: {batch_result_file}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RCA LLM-DA Agent")
    parser.add_argument("--dataset", type=str, default="Bank", help="Dataset name")
    parser.add_argument("--problem_number", type=int, default=1, help="Problem number")
    parser.add_argument("--batch", action="store_true", help="Run batch analysis")
    parser.add_argument("--max_problems", type=int, default=3, help="Maximum problems for batch analysis")
    parser.add_argument("--config", type=str, default="config.yaml", help="Configuration file")
    
    args = parser.parse_args()
    
    # åˆ›å»º Agent
    agent = RCALLMDAgent(config_path=args.config)
    
    if args.batch:
        # æ‰¹é‡åˆ†æ
        results = agent.batch_analysis(args.dataset, args.max_problems)
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r.get('status') != 'failed')
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æå®Œæˆ: {successful}/{len(results)} ä¸ªé—®é¢˜æˆåŠŸ")
        
    else:
        # å•é—®é¢˜åˆ†æ
        result = agent.run_rca_analysis(args.dataset, args.problem_number)
        
        if result.get('status') != 'failed':
            print(f"\nâœ… åˆ†æå®Œæˆ")
            print(f"   æ ¹å› æœåŠ¡: {result.get('root_service', 'unknown')}")
            print(f"   æ ¹å› åŸå› : {result.get('root_reason', 'unknown')}")
            print(f"   æ ¹å› æ—¶é—´: {result.get('root_time', 'unknown')}")
            print(f"   ç½®ä¿¡åº¦: {result.get('confidence', 0.0):.3f}")
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥: {result.get('error', 'unknown error')}")


if __name__ == "__main__":
    main()
